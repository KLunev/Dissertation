\chapter*{Введение}							% Заголовок
\addcontentsline{toc}{chapter}{Введение}	% Добавляем его в оглавление
\nocite{*}
Многие современные информационные системы, такие как социальные сети, блоговое и поисковые системы,  используют ключевые слова для описания содержащихся в них сущностей. Это значительно упрощает для пользователя поиск объектов системы, потому что позволяет с помощью запроса на естественном языке находить документы различной природы: текстовые документы, изображения, видеозаписи - любой объект, которому был приписан набор ключевых слов. Многие исследователи занимались задачами анализа ключевых слов в областях  кластеризации, визуализации и классификации,  индексации и поиска похожих объектов.


% близость слов
В рамках поиска подходов к решению рассматриваемой задачи проведены поисковые исследования. 


Одной из первых метрик похожести двух слов в компьютерных науках является расстояние Левенштейна (редакторское расстояние, \cite{leven}). Это метрика подсчитывает сколько необходимо произвести добавлений, удалений или замен одного символа на другой, чтобы из одной строки получить вторую. Традиционно этот алгоритм используется для исправления опечаток: для введенного слова можно найти ближайшие по этой метрике слова из фиксированного словаря. Существует ряд более сложных версий алгоритма, в числе которых алгоритм Демерау-Левенштейна \cite{leven_dem}. Усовершенствование этого алгоритма заключается в том, что дополнительно используется четвертая операция транспозиции двух соседних символов. Более продвинутые версии алгоритма используют настроенную таблицу стоимостей каждой из операции. Авторы \cite{learn_leven} с помощью разработанных алгоритмов и обучающей выборки определяют стоимость замены одного символа на другой (а также добавления и удаления каждого символа). Авторы высказывают гипотезу о том, что различные замены символов не должны иметь один и тот же вес: если человек опечатался, то вероятно, что он ввел символ, который находится близко к правильному символу на клавиатуре. Такая замена не должна сильно влиять на общее расстояние. Другим примером важности дифференцированного взвешивания замен могут являться безударные гласные: люди чаще путают при написании пару букв <<а>> и <<о>>, чем, например, пару букв <<а>> и <<е>>. Важным достоинством такого алгоритма является то, что слово и его транслитерированная версия (например, <<компьютер>>-<<computer>>) становятся близки по данному расстоянию. Недостаток алгоритма в том, что он требует обучающую коллекцию различных написаний одного слова.
Следующий важный этап развития идеи редакторского расстояния заключается в использовании контекста. В работе \cite{context_leven} авторы настраивают стоимости переходов между символами с учетом контекста. Выдвигается гипотеза о том, что стоимость замены одного символа на другого может сильно зависеть от символов, которые окружают эти два символа. Например, удаление символа <<ь>> более обоснованно в конце глаголов, так как ошибки <<ться>>/<<тся>> популярны и вероятнее всего речь идет об одном и том же слове. Как и в обозначенной выше работе, данный алгоритм требует обучающую выборку, но в данном случае ее размер должен быть значительно больше, поскольку число параметров растет экспоненциально с увеличением значения контекста.

%Другая идея в области использования мер близости на строках для определения смысловой близости -- использование 
Другим направлением исследований в области определения смысловой близости пары слов является использование фонетической информации рассматриваемых слов. Примером таких работ могут служить \cite{soundex,phone_sim}. Работы опираются на гипотезу о том, что похожие слова могут звучать одинаково. Авторы первой работы по слову строят его короткий код таким образом, чтобы различные слова с одним кодом звучали похоже. Авторы второй работы предлагают различные алгоритмы определения фонетической близости, в том числе они используют расстояние Левенштейна на фонемах для рассматриваемых слов.
%, авторы которых при использовании мер близости на строках для определения смысловой близости между понятиями

Базовыми методами определения близости пары понятий является сбор информации о совместной встречаемости слов внутри одного набора. Факт появления пары слов в одном предложении или тексте может быть важным сигналом для определения смысловой близости. Данные методы разбираются в \cite{freq_1,freq_2,pmi}. Более продвинутые алгоритмы основаны на вычислении взаимной информации (Pointwise mutual information), введенной авторами \cite{pmi}, которая выносит решение об уровне близости по совместной встречаемости пары слов в документах, но пессимизирует значение, если слова, для которых вычисляется близость, встречаются слишком часто в представленных данных по-отдельности. Таким образом, высокое значение семантической близости имеют пары слов, которые часто встречаются вместе и редко поодиночке. Для того, чтобы такая мера адекватно представляла близость между понятиями, необходимы корпусы огромных значений: чем меньше раз понятия встретились в документах, тем сильнее каждый отдельный случай совместного появления влияет на данную метрику. Зачастую эта особенность приводит к тому, что наиболее близкими парами в смысле этой меры близости являются те, которые встретились единственный раз в одном общем документе. Для того, чтобы уменьшить негативный эффект на практике, принято исключать пары слов, которые встретились в корпусе меньше некоторого порогового значения. Другой способ обойти сложившуюся трудность в ином определении вероятностей появления каждого из слов, а также вероятности их совместного появления. Для этого служат методики сглаживания вероятности, принятые в области построения языковых моделей. Основные способы сглаживания представлены в работе \cite{lm}. Также существуют усовершенствования PMI меры различными эвристическими предположениями: усредненная и средневзвешенная взаимная информация (average and weighted average mutual information), рассмотренные, соответственно, в \cite{avg_pmi} и \cite{w_avg_pmi}; контекстная усредненная взаимная информация (contextual average mutual information), введенная в \cite{context_pmi}; нормированная взаимная информация (normalized mutual information), введенная в \cite{npmi}, квадратичная и кубическая взаимная информация (PMI2 и PMI3), рассмотренные в \cite{pmi23}. 
В другой работе \cite{search_eng} вопрос вычисления семантической близости решается с помощью поисковых систем. Программа запрашивает пару сравниваемых слов через открытый API и получает совместную и индивидуальные частоты встречаемостей слов в интернете. На основе этой информации подсчитывается уровень похожести слов друг на друга. Очевидным недостатком такого подхода является ограниченная поисковой системой пропускная способность (количество запросов в единицу времени) и общее количество запрос.
Различные вариации PMI-метрик являются, по сути, вероятностными методами, поскольку подразумевают вычисления оценки вероятности встретить каждое из понятий, а также эту пару понятий совместно внутри одного текста. Существуют и другие вероятностные методы сравнения пары слов естественного языка. Например, может быть использован $\chi^2$ критерий и тест отношения правдоподобия. Способ применения данных методов описан в \cite{freq_est_overview}
Важной особенностью в применении PMI-подобных метрик к набору текстов является то, что они не показывают смысловую близость между понятиями в явном виде, а скорее определяют коллокации: <<Российская Федерация>>, <<крейсер Аврора>>, <<завод имени Кирова>>, <<средний класс>>, <<пластическая операция>> и другие. Тем не менее, знание того, что совместная встречаемость пары понятий внутри одного множества слов (предложение, документ, набор ключевых слов, короткое описание объекта и т.д.) статистически значимо превосходит случаи их отдельных появлений, является важным фактором для определения в том числе и семантической близости между этими понятиями.

Многие подходы к решению задачи определения близости пары слов используют понятие нграммы.
Cимвольной/пословной нграммой называют последовательность фиксированной длины из определенного числа подряд идущих символов/слов. Символьные и пословные нграммы широко используются в различных задачах из области обработки естественного языка таких как построение языковых моделей \cite{ngrams_1,ngrams_2,ngrams_3,ngrams_4} и моделей машинного перевода \cite{ngrams_mt_1,ngrams_mt_2,ngrams_mt_3}. Недостатком нграммных моделей является так называемое проклятие размерности, которое в данном случае говорит о том, что при увеличении длины нграммы катастрофически быстро растет число возможных нграмм данного размера, а также параметров системы, что делает затруднительным их применение во многих случаях. Также при работе с нграммами необходимы объемы текстов огромных размеров. Если предметная область, в которой решается задача, является узкоспециальной, то получение данных достаточного объема зачастую является невозможным.

В работе \cite{ngrams_sim} авторами предложены различные метрики близости на основе нграммного представления слов.

В работе \cite{Albatineh2011} авторы используют  меру Жаккара для определение близости пары ключевых слов: каждому ключевому слову ставится в соответствии множество понятий и для определения близости вычисляются размеры объединения и пересечения этих множеств. Отмечается, что как только словам в однозначное соответствие поставлены некоторые множества, сразу становится возможным вычисление близости на основании различных мер близости пары множеств. Помимо меры Жаккара, существует чуть менее популярная мера Серенсена (\cite{dice_1}). Эти и другие меры близости на множествах подробно описаны в \cite{dist_between_sets}.

Авторы \cite{Shirude} для определения близости используют комбинацию три модели определения близости: нграммную модель, модель близости жаккара, а также модель векторного пространства. Последняя подразумевает представление слов в виде вектора определенной длины. Близость в свою очередь сводится к величине скалярного произведения векторов для двух слов. Эта модель детально описывается в \cite{vector_space}. Имея три функции близости, авторы вычисляют среднее по их значениям. Это приводит к тому, что такая композиция уменьшает влияние слабых сторон каждого отдельного алгоритма. Тем не менее такой наивный метод комбинирования может даже ухудшать результат, если входящие в нее модели демонстрируют низкое качество.

Важной особенностью алгоритмов, основанных на вычислении символьной нграммной близости, расстояния левенштейна и наибольшей общей подпоследовательности, является то, что такие методы не дают представления о смысловой близости между словами и способны определять близкие слова только по похожести написания. Это является серьезным недостатком для задач, в которых важна смысловая близость между понятиями. Примером такой задачи может быть разработка классической поисковой системы, где удачное добавление синонимов для слов запроса, сформулированного пользователем, может вылиться в более релевантную выдачу для этого запроса.  Несмотря на этот недостаток, данные методы могут быть применены внутри более сложных алгоритмов для повышения их качества.

Существует множество методов, использующих для вычисления близости внешние наборы данных. Такими наборами могут быть словари, семантические сети, тезаурусы или данные сети Веб. Важным источником знаний об отношениях между словами английского языка является семантическая сеть WordNet (\cite{wordnet}). Слова в данной сети могут быть связаны одним из нескольких отношений: гипероним, гипоним, <<имеет участника>> (факультет-профессор), <<является участником>> (пилот-экипаж), мероним, антоним. Также имеются лексические, антонимические, контекстные связи между словами. Для русского языка существует несколько аналогов: RussNet (\cite{russnet}), YARN (\cite{yarn, yarn_2}), RuThes (\cite{ruthes}), Russian WordNet (\cite{russian_wordnet}). Недостаток этих тезаурусов в их неполноте, а также в том, что некоторые из них не являются публично доступными. Чтобы посчитать близость по таким тезаурусам, строится дерево, в вершинах которого стоят слова (вершина в \textsc{WordNet} является синсетом - множеством слов, не отличимых по смыслу), а ребра указывают на отношение гиперонимии между парой вершин. Таким образом, в листьях дерева лежат узкоспециальные понятия, которые обобщаются их предками в дереве, в корне же лежит слово наиболее общего значения. Имея такое дерево, появляется возможность вычислять смысловую близость понятий по их взаимному расположению внутри этого дерева. Так авторы \cite{wordnet_sim_0} в своей формуле близости используют глубину наиболее конкретного по значению предка, а авторы  \cite{wordnet_sim_1} в дополнении к этому считают расстояние между вершинами. Чем больше расстояние между словами и чем глубже находится общий предок, тем меньше уровень смысловой похожести. В работах \cite{wordnet_hybrid_1,wordnet_hybrid_2} используются гибридные методы определения близости: расстояния и глубина в дереве, вероятности встречаемостей в корпусах, признаки, основанные на свойствах слов в рассматриваемых тезаурусах.

Еще одним открытым источником отношений между понятиями является интернет-энциклопедия wikipedia (www.wikipedia.org). Данная база позволяет эффективно использовать категории, ссылки, полные тексты и мета-данные статей для извлечения семантической информации о словах. Авторы \cite{wiki} строят различные меры близости, опираясь на тексты статей и представляя сравниваемые понятия в виде векторов определенной длины. В \cite{wiki_2} автор использует данные википедии (в частности используются информация о ссылках между статьями) и разработанные им метрики близости слов для решения задачи снятия лексической неоднозначности. 

С развитием вычислительной техники все большей и большей популярностью начинают пользоваться методы, основанные на обучении нейронный сетей. Одним из самых известных методов определения семантической близости является модель \textsc{word2vec} (\cite{word2vec}). Данная модель представляет собой нейронную сеть, на вход которой подаются огромные корпуса текстовых данных. Задачей обучения является построение такого векторного представления для текущего слова (\textsc{word embeddings}), которое максимально точно способно предсказать рядом стоящие в тексте слова. Обученная модель строит векторное пространство, обладающее рядом полезных свойств, которые в наше время широко используются для решения многих задач естественного языка, связанных с семантической информацией. Одним из таких свойств - семантическая близость понятий, векторные представления которых похожи. Таким образом любую пару слов из словаря можно сравнить, использую, например, косинусное расстояние между векторами.

Мощной моделью построения векторных представлений для слов является модель GloVe, которая строит матрицу частотностей встречаемостей слов во всех возможных контекстах. Далее используются методы уменьшения размерности пространства, которые оставляют только наиболее значимые компоненты в разложении. В то время, как \textsc{Word2Vec} является предиктивной моделью, \textsc{GloVe} представляет собой модель на основе подсчета статистики.

Различные методы векторного представления описаны и протестированы на открытых источниках в работе \cite{embed_1}. Несмотря на высокое качество определения семантической близости моделей, использование полнотекстовой информации существенно ограничивает область применения данных методов, посколько для многих прикладных задач не имеется достаточно много текстовых данных. Возникает трудность при работе в узкоспециальных областях: модели, обученные на корпусах общего назначения, не могут улавливать особенности таких областей. Использование текстовых данных из рассматриваемой области для обучения ведет к неправильной настройке параметров модели и недообучению, по причине недостатка этих самых данных. Это выливается в низкий уровень качества моделей. 

При исследовании области семантической близости пары ключевых слов возникает дополнительная информация о наборах, в которые входят рассматриваемые слова. Помимо этого зачастую для набора известен также объект, к которому этот набор приписан. Авторы \cite{folk} вводят понятие фолксономии. Фолксономией называется кортеж $(U, T, R, Y)$, где $U,T$ и $R$ - конечные множества, элементами которых служат, соответственно, пользователи, ключевые слова и ресурсы. $Y$ - тернарное отношение между ними, т.е. $Y  \subseteq U \times T \times R$. Постом называется тройка $(u, T_{ur}, r)$, где $u \in U, r \in R$, $T_{ur}$ - непустое множество ключевых слов такое, что $T_ur  \coloneqq {t \in T | (u, t, r) \in Y}$. Авторы \cite{folk_2} считают близость между ключевыми словами несколькими способами. Первый способ заключается в построении меры близости по статистике совместной встречаемости пары ключевых слов. Второй способ предполагает построение векторного пространства для каждого слова. На $i-$ой позиции стоит количество документов, в которые одновременно входит рассматриваемое ключевое слово и $i-$ое. Далее мера близости вводится как косинусное расстояние между векторами в этом пространстве. Последний способ, который описывается в \cite{folk} подсчитывает меру, подобную мере \textsc{PageRank} (\cite{pagerank}) для документов в сети Веб.
   В [PAPER] авторы проводят вычисление близости, основываясь на теоретико-графовых алгоритмах. Рассматривается граф, вершинами которого являются ключевые слова, а ребра показывают факт принадлежности пары слов одному набору. Далее по построенному графу для пары вершин  вычисляются различные характеристики: расстояние, количество кратчайших путей, различные графовые меры близости. Недостатком описанной в статье модели является тот факт, что смысловая близость между парой слов стремительно падает при увеличении расстояния в графе. Это происходит по той причине, что набор ключевых слов далеко не всегда состоит из близких по смыслу слов. Поэтому при переходе от одной вершины к другой, вероятность исходной вершины быть близкой по смыслу к другой стремительно уменьшается. В работе [word2vec] для определения контекстной близости между словами естественного языка авторы обучают по корпусу текстов нейронную сеть, представляющую каждое слово в виде вектора не очень большой длины. Имея такое представление, уровень близости пары слов может быть вычислен, как мера близости между векторами, например, с помощью косинусной меры. В работах [] представлены подходы решения задачи с помощью классических методов машинного обучения с учителем. По паре текстов вычисляются вручную разработанные факторы, которые, по мнению авторов, сильнее всего влияют на уровень близости. После чего на этих факторах и обучающем множестве пар документов тренируется модель машинного обучения. В более старых работах близость [] вычисляется при помощи статистической меры TFIDF. Каждому слову документа в соответствии ставится число, которое тем больше, чем чаще это слово встречается в данном документе и реже в других документах. 


%близость наборов
Существующие методы имеют ряд недостатков по отношению к решаемой в данной работе задаче. Основная из них - отстутствие наборов данных достаточного объема. Для качественного обучения нейронной сети необходимы миллионы примеров полноценных текстов, в то время как наборы ключевых слов, как правило, состоят лишь из нескольких слов. Доступ к ресурсам поисковых систем является ограниченным и не имея постоянного доступа к ним, сложно получить хорошие результаты. Сложность применения машинного обучения к такого рода задачам - в отсутствии достаточно больших обучающих выборок для тренировки.

В рамках данной работы представлены методы определения близости по корпусу наборов ключевых слов, также опирающиеся на методы из теории графов. Значительным улучшением является построение второго графа ключевых слов, основанного на контекстной близости пары слов. В следующих далее разделах дано определение контекстной близости для пары ключевых слов, а также представлены методы построения такого графа. После чего показан алгоритмы семантической кластеризации, основанные на введенной мере близости слов. В разделе [?] представлены тестовые данные, результаты экспериментов программных реализаций алгоритмов.


\newcommand{\actuality}{}
\newcommand{\progress}{}
\newcommand{\aim}{{\textbf\aimTXT}}
\newcommand{\tasks}{\textbf{\tasksTXT}}
\newcommand{\novelty}{\textbf{\noveltyTXT}}
\newcommand{\influence}{\textbf{\influenceTXT}}
\newcommand{\methods}{\textbf{\methodsTXT}}
\newcommand{\defpositions}{\textbf{\defpositionsTXT}}
\newcommand{\reliability}{\textbf{\reliabilityTXT}}
\newcommand{\probation}{\textbf{\probationTXT}}
\newcommand{\contribution}{\textbf{\contributionTXT}}
\newcommand{\publications}{\textbf{\publicationsTXT}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам


\textbf{Объем и структура работы.} Диссертация состоит из~введения, четырёх глав, заключения и~двух приложений.
%% на случай ошибок оставляю исходный кусок на месте, закомментированным
%Полный объём диссертации составляет  \ref*{TotPages}~страницу с~\totalfigures{}~рисунками и~\totaltables{}~таблицами. Список литературы содержит \total{citenum}~наименований.
%
Полный объём диссертации составляет
\formbytotal{TotPages}{страниц}{у}{ы}{}, включая
\formbytotal{totalcount@figure}{рисун}{ок}{ка}{ков} и
\formbytotal{totalcount@table}{таблиц}{у}{ы}{}.   Список литературы содержит  
\formbytotal{citenum}{наименован}{ие}{ия}{ий}.
