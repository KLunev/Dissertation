\chapter{Методы и средства анализа информации с использованием ключевых слов} \label{chapt_related_work}
% SYNOPSIS_0 >>>
В данной главе рассматриваются известные подходы к решению задач, направленных на изучение семантической информации, проводится их анализ, выявляются недостатки. После чего автором описывается использованная в настоящей диссертации методология. Эта методология призвана решить недостатки в существующих моделях определения смысловой близости объектов информационных систем по их ключевым словам.

В первом разделе проводится библиографический обзор существующих методов, выделяются сильные и слабые стороны моделей. Анализу подвергаются работы, в которых исследуются семантическая близость для пар слов и пар коротких предложений естественного языка. В частности, обозреваются работы, связанные с анализом ключевых слов.

Второй раздел содержит в себе выводы из обзора и указывает на недостатки рассмотренных методов и на причины сложностей применения этих методов к решаемым в данной работе задачам.

В заключающем разделе главы вводится методология, используемая автором в последующих главах для разработки моделей семантической близости. Показывается целесообразность и необходимость рассмотренной методологии, ее преимущества в сравнении с уже существующими. Следует также отметить, что отдельные пункты методологии представляют собой самостоятельный интерес.
% <<< SYNOPSIS_0

\section{Библиографический обзор}

С целью анализа эффективности уже существующих и поиска новых подходов к решению рассматриваемой задачи автором проведены библиографические поисковые исследования, результаты которых представлены в настоящей разделе.

\subsection{Методы определения близости между парой слов естественного языка}

% близость слов

Существует большое число общих методов определения похожести пары слов естественного языка. Их можно разделить на методы, не использующие или использующие дополнительные источники информации. 

\textbf{Методы, использующие только информацию о написании слов.} К числу таких относятся исторически наиболее ранние и наивные подходы, которые вычисляют меру близости, не используя никакой дополнительной информации о словах, кроме их непосредственного написания. Одной из основных метрик данного типа является расстояние Левенштейна  (редакторское расстояние, \cite{leven}). Эта метрика подсчитывает количество необходимых операций добавления, удаления или замены одного символа на другой, чтобы из одной строки получить вторую. Традиционно этот алгоритм используется для исправления опечаток: для введенного слова можно найти ближайшие по этой метрике слова из фиксированного словаря. 

Существует ряд более сложных версий алгоритма, в числе которых алгоритм Демерау-Левенштейна \cite{leven_dem}. Усовершенствование этого алгоритма заключается в том, что дополнительно используется четвертая операция транспозиции двух соседних символов.

Несмотря на простоту описанных выше методов, исследования в этом направлении ведутся до сих пор. Данный класс методов может использоваться в более сложных и совершенных моделях в качестве дополнительных источников для определения близости. Примером более сложной модели в данном направлении является модель редакторского расстояния с настроенными стоимостями для операций вставки, удаления и замены символов.
Авторы \cite{learn_leven} с помощью разработанных алгоритмов и обучающей выборки определяют стоимость замены одного символа на другой (а также добавления и удаления каждого символа). Авторы высказывают гипотезу о том, что различные замены символов не должны иметь один и тот же вес: если человек опечатался, то весьма вероятно, что он ввел символ, который находится близко к правильному символу на клавиатуре. Такая замена не должна сильно влиять на общее расстояние метрике. Другим примером важности индивидуального подбора весов замен под каждый символ могут являться безударные гласные: люди чаще путают при написании пару букв <<а>> и <<о>>, чем, например, пару букв <<а>> и <<е>>. Важным достоинством такого алгоритма является то, что слово и его транслитерированная версия (например, <<компьютер>>-<<computer>>) становятся близки по данному расстоянию. В то же время, недостатком является необходимость обучающей коллекцию различных написаний одного слова.

Следующий важный этап развития идеи редакторского расстояния заключается в использовании контекста. В работе \cite{context_leven} авторы настраивают стоимости переходов между символами с учетом контекста. Выдвигается гипотеза о том, что стоимость замены одного символа на другого может сильно зависеть от символов, которые стоят рядом с заменяемым символов и символом-заменителем. Например, удаление символа <<ь>> более обоснованно в конце глаголов, так как ошибки <<ться>>/<<тся>> частотны и вероятнее всего подразумевал одно из слов, написав другое. Как и в предыдущей работе, данный алгоритм требует обучающую выборку, но в данном случае ее размер должен быть значительно больше, поскольку число параметров растет экспоненциально с увеличением размера рассматриваемого контекста.

Еще одной разновидностью метрик на строках является расстояние Джаро — Винклера (\cite{jaro1,jaro2}). Эта метрика подсчитывает минимальное число односимвольных преобразований, которое необходимо для того, чтобы изменить одно слово в другое и использовалась для сравнения написаний имен в Бюро переписи населения США.

Ряд авторов рассматривают слово как множество символов (или как множество символьных n-грамм - последовательностей из n подряд идущих символов) и далее определяют близость между словами, как близость между соответствующими множествами, либо как близость между соответсвующими векторами, на $i$-ой позиции в которой стоит единица, если данная n-грамма присутствует в слове и ноль - в противном случае. Примером метрики на векторах может являться алгоритм q-grams (\cite{qgrams}), в ходе работы которого подсчитывается число совпавших n-грамм. Другими метриками могут служить,  расстояние Жаккара или косинусное расстоения, приведенное, например, в \cite{sim_metr}.

%Другая идея в области использования мер близости на строках для определения смысловой близости -- использование 
Другим направлением исследований в области определения смысловой близости пары слов является использование фонетической информации рассматриваемых слов. Примерами таких работ могут служить \cite{soundex,phone_sim}. Работы опираются на гипотезу о том, что похожие слова могут звучать одинаково. Авторы первой работы по слову строят его короткий код таким образом, чтобы различные слова с одним кодом звучали похоже. Авторы второй работы предлагают различные алгоритмы определения фонетической близости, в том числе они используют расстояние Левенштейна на фонемах для рассматриваемых слов.
%, авторы которых при использовании мер близости на строках для определения смысловой близости между понятиями

Таким образом, данное направление позволяет строить метрики близости, основанные исключительно на написании конкретных слов. Данный класс методов позволяет достаточно эффективно решать задачу исправления опечаток, но имеет множество недостатков. Основной из них заключается в том, что при его использовании не учитывается семантика слов. Это обстоятельство существенно сужает круг прикладных задач, для решения которых эти методы можно было бы применить. 

\textbf{Методы, использующие дополнительные знания о словах}. Существенного улучшения качества определения близости между парой ключевых слов можно добиться, используя дополнительные знания о словах. Это могут быть тексты, в которых слова употреблены, коллекции ключевых слов, информация об объектах, к которым эти слова приписаны, вручную составленные тезаурусы и словари. Каждое из этих направлений имеет свои преимущества и недостатки, анализ которых приведен далее.

Одно из направлений определения близости пар ключевых слов с использованием вспомогательных данных - \textbf{изучение частот использования} рассматриваемых слов в различных коллекциях текстовых документов. Базовым методом определения близости пары ключевых слов в рамках таких исследований является сбор информации о совместной встречаемости слов внутри одного набора. Факт появления пары слов в одном предложении или тексте может быть важным сигналом для определения смысловой близости. Методы, основанные на этой идее, разбираются в \cite{freq_1,freq_2,pmi}. Более совершенные на этом направлении алгоритмы основаны на вычислении взаимной информации (Pointwise mutual information, PMI), введенной авторами [7]. Использование таких алгоритмов позволяет получить решение об уровне близости пар слов  не только по их совместной встречаемости, но и путем учета частоты встречаемости каждого из слов в коллекции. Согласно этой метрике, высокое значение семантической близости имеют пары слов, которые часто встречаются вместе и редко поодиночке. 

Недостатком описанных выше статистических методов является необходимость сбора коллекции данных большого размера, поскольку значения PMI и подобных ему статистических метрик сильно неустойчивы (\cite{lm}).  Зачастую эта особенность приводит к тому, что наиболее близкими парами в смысле этой меры близости являются те, которые встретились единственный раз в одном общем документе. Для того, чтобы уменьшить негативный эффект на практике, принято исключать пары слов, которые встретились в корпусе меньше некоторого порогового значения. Другой способ обойти сложившуюся трудность в ином определении вероятностей появления каждого из слов, а также вероятности их совместного появления. Для этого служат методики сглаживания вероятности, принятые в области построения языковых моделей. Основные способы сглаживания представлены в работе \cite{lm}. Также существуют усовершенствования PMI меры различными эвристическими предположениями: усредненная и средневзвешенная взаимная информация (average and weighted average mutual information), рассмотренные, соответственно, в \cite{avg_pmi} и \cite{w_avg_pmi}; контекстная усредненная взаимная информация (contextual average mutual information), введенная в \cite{context_pmi}; нормированная взаимная информация (normalized mutual information), введенная в \cite{npmi}, квадратичная и кубическая взаимная информация (PMI2 и PMI3), рассмотренные в \cite{pmi23}.  В этих публикациях отмечается, что сам контекст, в котором употребляются слова, используется в самом примитивном виде, а именно, в данном контексте проверяется факт наличия обоих рассматриваемых слов. Остальные слова контекста никак не учитываются, что является существенным недостатком описанных выше подходов.

В другой работе \cite{search_eng} вопрос вычисления семантической близости решается с помощью поисковых систем. Программа запрашивает пару сравниваемых слов через открытый API и получает совместную и индивидуальные частоты встречаемостей слов в интернете. На основе этой информации подсчитывается уровень похожести слов друг на друга. Очевидным недостатком такого подхода является ограниченная поисковой системой пропускная способность (количество запросов в единицу времени) и общее количество запрос.

Различные вариации PMI-метрик являются, по сути, вероятностными методами, поскольку подразумевают вычисления оценки вероятности встретить каждое из понятий, а также эту пару понятий совместно внутри одного текста. Существуют и другие вероятностные методы сравнения пары слов естественного языка. Например, может быть использован $\chi^2$ критерий и тест отношения правдоподобия. Способ применения данных методов описан в \cite{freq_est_overview}

Важной особенностью в применении PMI-подобных метрик к набору текстов является то, что они не показывают смысловую близость между понятиями в явном виде, а скорее определяют коллокации: <<Российская Федерация>>, <<крейсер Аврора>>, <<завод имени Кирова>>, <<средний класс>>, <<пластическая операция>> и другие. Тем не менее, знание того, что совместная встречаемость пары понятий внутри одного множества слов (предложение, документ, набор ключевых слов, короткое описание объекта и т.д.) статистически значимо превосходит случаи их отдельных появлений, является важным фактором для определения в том числе и семантической близости между этими понятиями.

Многие подходы к решению задачи определения близости пары слов используют понятие n-граммы.
Cимвольной/пословной n-граммой называют последовательность фиксированной длины из определенного числа подряд идущих символов/слов. Символьные и пословные n-граммы широко используются в различных задачах из области обработки естественного языка таких как построение языковых моделей \cite{ngrams_1,ngrams_2,ngrams_3,ngrams_4} и моделей машинного перевода \cite{ngrams_mt_1,ngrams_mt_2,ngrams_mt_3}. Недостатком n-граммных моделей является так называемое проклятие размерности, которое в данном случае свидетельствует о том, что при увеличении длины n-граммы катастрофически быстро растет число возможных n-грамм данного размера, а также параметров системы, что делает затруднительным их применение во многих случаях. Также при работе с n-граммами необходимы объемы текстов огромных размеров. Если предметная область, в которой решается задача, является узкоспециальной, то получение данных достаточного объема зачастую является невозможным.

В работе \cite{ngrams_sim} авторами предложены различные метрики близости на основе n-граммного представления слов. В работе \cite{Albatineh2011} авторы используют  меру Жаккара для определение близости пары слов: каждому слову ставится в соответствии множество понятий и для определения близости вычисляются размеры объединения и пересечения этих множеств. Отмечается, что как только словам в однозначное соответствие поставлены некоторые множества, сразу становится возможным вычисление близости на основании различных мер близости пары множеств. Помимо меры Жаккара, существует чуть менее популярная мера Серенсена (\cite{dice_1}). Эти и другие меры близости на множествах подробно описаны в \cite{dist_between_sets}.

Авторы \cite{Shirude} для определения близости используют комбинацию из трех моделей определения близости: n-граммную модель, модель близости жаккара, а также модель векторного пространства. Последняя подразумевает представление слов в виде вектора определенной длины. Близость в свою очередь сводится к величине скалярного произведения векторов для двух слов. Эта модель детально описывается в \cite{vector_space}. Имея три функции близости, авторы вычисляют среднее по их значениям. Это приводит к тому, что такая композиция уменьшает влияние слабых сторон каждого отдельного алгоритма. Тем не менее такой наивный метод комбинирования может даже ухудшать результат, если входящие в нее модели демонстрируют низкое качество.

Важной особенностью алгоритмов, основанных на вычислении символьной n-граммной близости, расстояния левенштейна и наибольшей общей подпоследовательности, является то, что такие методы не дают представления о смысловой близости между словами и способны определять близкие слова только по похожести написания. Это является серьезным недостатком для задач, в которых важна смысловая близость между понятиями. Примером такой задачи может быть разработка классической поисковой системы, где удачное добавление синонимов для слов запроса, сформулированного пользователем, может вылиться в более релевантную выдачу для этого запроса.  Несмотря на этот недостаток, данные методы могут быть применены внутри более сложных алгоритмов для повышения их качества.

Существует класс методов, решающих задачу семантической близости пар слов с помощью \textbf{готовых тезаурусов, словарей или других семантических сетей}. Важным источником знаний об отношениях между словами английского языка является семантическая сеть WordNet (\cite{wordnet}). Слова в данной сети могут быть связаны одним из нескольких отношений: гипероним, гипоним, <<имеет участника>> (факультет-профессор), <<является участником>> (пилот-экипаж), мероним, антоним. Также имеются лексические, антонимические, контекстные связи между словами. Для русского языка существует несколько аналогов: RussNet (\cite{russnet}), YARN (\cite{yarn, yarn_2}), RuThes (\cite{ruthes}), Russian WordNet (\cite{russian_wordnet}).  Чтобы посчитать близость по таким тезаурусам, строится дерево, в вершинах которого стоят слова (вершина в \textsc{WordNet} является синсетом - множеством слов, не отличимых по смыслу), а ребра указывают на отношение гиперонимии между парой вершин. Таким образом, в листьях дерева лежат узкоспециальные понятия, которые обобщаются их предками в дереве, в корне же лежит слово наиболее общего значения. Имея такое дерево, появляется возможность вычислять смысловую близость понятий по их взаимному расположению внутри этого дерева. Так авторы \cite{wordnet_sim_0} в своей формуле близости используют глубину наиболее конкретного по значению предка, а авторы  \cite{wordnet_sim_1} в дополнении к этому считают расстояние между вершинами. Чем больше расстояние между словами и чем глубже находится общий предок, тем меньше уровень смысловой похожести. В работах \cite{wordnet_hybrid_1,wordnet_hybrid_2} используются гибридные методы определения близости: расстояния и глубина в дереве, вероятности встречаемостей в корпусах, признаки, основанные на свойствах слов в рассматриваемых тезаурусах. Недостаток тезаурусных подходов в их неполноте, а также в том, что некоторые из них не являются публично доступными. Кроме того, тезаурусы обычно охватывают общий домен и существует мало словарей для специфических областей.

Еще одним открытым источником отношений между понятиями является интернет-энциклопедия wikipedia (www.wikipedia.org). Данная база позволяет эффективно использовать категории, ссылки, полные тексты и мета-данные статей для извлечения семантической информации о словах. Авторы \cite{wiki} строят различные меры близости, опираясь на тексты статей и представляя сравниваемые понятия в виде векторов определенной длины. В \cite{wiki_2} автор использует данные википедии (в частности используются информация о ссылках между статьями) и разработанные им метрики близости слов для решения задачи снятия лексической неоднозначности. 

С развитием вычислительной техники большой популярностью начинают пользоваться методы, основанные на обучении \textbf{нейронный сетей}. Одними из самых известных методов определения семантической близости слов является модели \textsc{word2vec} (\cite{word2vec}), \textsc{GloVe} (\cite{glove}) и \textsc{StarSpace} (\cite{starspace}) . Модель \textsc{word2vec} представляет собой нейронную сеть, на вход которой подаются большие корпуса текстовых данных. Задачей обучения является построение такого векторного представления для текущего слова (\textsc{word embeddings}), которое максимально точно способно предсказать рядом стоящие в тексте слова. Обученная модель строит векторное пространство, обладающее рядом полезных свойств, которые в наше время широко используются для решения многих задач естественного языка, связанных с семантической информацией. Одним из таких свойств - семантическая близость понятий, векторные представления которых похожи. Таким образом любую пару слов из словаря можно сравнить, использую, например, косинусное расстояние между векторами.

Мощной моделью построения векторных представлений для слов является модель GloVe, которая строит матрицу частотностей встречаемостей слов во всех возможных контекстах. Далее используются методы уменьшения размерности пространства, которые оставляют только наиболее значимые компоненты в разложении. В то время, как \textsc{Word2Vec} является предиктивной моделью, \textsc{GloVe} представляет собой модель на основе подсчета статистики.

\hl{Модель StarSpace является более современной моделью построения векторных представлений и обобщает идеи, заложенные в word2vec. Использование StarSpace дает возможность построения векторных представлений для слов, предложений или целых документов. Кроме того, возможно построение представлений для графовых данных.}

\hl{Рассматривая известные подходы построения векторных представлений, нельзя не упомянуть про языковую модель BERT (\cite{bert}). Предобученные текстовые представления, полученные этой моделью, могут быть дообучены для решения конкретных задач обработки естественного языка. Примерами таких задач служат задачи разработки вопросно-ответной системы, построения автоматических переводчиков, создания модели анализа тональности текста, распознавание именованных сущностей. В ряде задач обработки естественного языка с помощью получены наилучшие из известных результаты.}

Еще один способ построения представлений показан в работе \cite{spmi}. Подход основан на предварительном вычислении меры \emph{PMI} (\cite{pmi}) и построением матрицы, в ячейках которой лежит значение PMI для пары рассматриваемых слов. После чего к этой матрице применяется SVD-разложение (\cite{svd}), что значительно понижает ее размерность. Строки новой матрицы пониженной размерности являются векторными представлениями для слов. Близость считается с помощью косинусного расстояния.

Различные методы векторного представления описаны и протестированы на открытых источниках в работе \cite{embed_1}. Данные методы являются очень эффективными, но требуют огромные наборы данных для обучения моделей. Это обстоятельство делает их неприменимыми к задачам, в которых полные тексты документов недоступны. К их числу которых принадлежит задача определения семантической близости пары ключевых слов научных публикаций. Эти методы зачастую определяют также контекстную близость, по определению которой два слова близки, если они встречаются в похожих контекстах. Во многих практических задачах подобного эффекта использования метрики близости хочется избежать, поскольку, например, слова “Математика” и “Физика” могут встречаться в одних и тех же контекстах, но как пара ключевых слова для научных публикаций эти слова явно не являются  семантически близкими.

Несмотря на высокое качество определения семантической близости моделей, использование полнотекстовой информации существенно ограничивает область применения данных методов, посколько для многих прикладных задач не имеется достаточного количества текстовой информации. Возникает трудность при работе в узкоспециальных областях: модели, обученные на корпусах общего назначения, не могут улавливать особенности таких областей. Использование текстовых данных из рассматриваемой области для обучения ведет к неправильной настройке параметров модели и недообучению, по причине недостатка этих самых данных. \hl{Это приводит к низкому уровню качества моделей.}

Существует группа При исследовании области семантической близости пары ключевых слов возникает дополнительная информация о наборах, в которые входят рассматриваемые слова. Помимо этого зачастую для набора ключевых слов известен также объект, к которому этот набор приписан. Авторы \cite{folk} вводят понятие \textbf{фолксономии}. Фолксономией называется кортеж $(U, T, R, Y)$, где $U,T$ и $R$ - конечные множества, элементами которых служат, соответственно, пользователи, ключевые слова и ресурсы. $Y$ - тернарное отношение между ними, т.е. $Y  \subseteq U \times T \times R$. \hl{Постом пользователя $u$ в ресурсе $r$} называется тройка $(u, T_{ur}, r)$, где $u \in U, r \in R$, $T_{ur}$ - непустое множество ключевых слов такое, что $T_{ur}  \coloneqq \{t \in T | (u, t, r) \in Y\}$. Авторы \cite{folk_2} считают близость между ключевыми словами несколькими способами. Первый способ заключается в построении меры близости по статистике совместной встречаемости пары ключевых слов. Второй способ предполагает построение векторного пространства для каждого слова. На $i-$ой позиции стоит количество документов, в которые одновременно входит рассматриваемое ключевое слово и $i-$ое. Далее мера близости вводится как косинусное расстояние между векторами в этом пространстве. Последний способ, который описывается в \cite{folk} подсчитывает меру, подобную мере \textsc{PageRank} (\cite{pagerank}) для документов в сети Веб.

\subsubsection{Недостатки существующих решений} \label{drawbacks}
Основные недостатки наиболее востребованных на практике существующих подходов для определения семантической близости пары слов естественного языка заключаются в следующем.
\begin{itemize}
    \item Сложные модели определения близости требуют больших объемов полнотекстовой информации. Информационно-аналитические системы зачастую имеют на порядки меньшие объемы данных, что делает невозможным обучение таких моделей.
    \item Использование готовых моделей не позволяет в должной мере учитывать семантическую специфику конкретной информационной системы. Другими словами, пара семантически близких понятий одной системы может не являться таковой во второй. Например, пара слов <<вычислительная математика>> и <<теория чисел>> могут считаться похожими в системах общего назначения, как два направления в математике. В то же время, для более узкоспециальной наукометрической системы эти понятия не должны быть слишком близки семантически.
    \item Простые модели не способны восстанавливать сложные семантические связи в данных.
    \item Существующие модели не используют в достаточной мере дополнительную информацию об отношениях между сущностями системы.
\end{itemize}

%\subsection{Методы определения близости между парой наборов ключевых слов}
\subsection{Методы определения близости между объектами в графах знаний} \label{kg_subsection}
Переходя на более высокий и целостностный уровень, можно отметить, что некоторые исследования в области семантической близости были проделаны и на этапе определения близости объектов информационной системы. Более того, исследователями зачастую решалась общая задача восстановления отношений между объектами, что не всегда равносильно наличию смысловой близости между ними. 

Возможны различные способы хранения данных информационной системы. Одним из самых современных на момент написания является граф знаний. Графы знаний - ориентированные графы с подписанными ребрами (отношениями) между вершинами (сущностями). При этом количество возможных типов отношений может быть огромным и исчисляться тысячами. Существует другое название для графов знаний - \emph{гетерогенная информационная сеть (heterogeneous information network, HIN)}. Этот термин представляется удобным в ситуациях, когда необходимо отличать системы с многочисленными типами отношений между сущностями (гетерогенные) от систем, в которых сущности связаны единственным типом отношения (гомогенные информационные сети).

Примерами моделей в области гомогенных информационных сетей являются модели \emph{SimRank} (\cite{simrank}), \emph{персонализированный PageRank} (\cite{ppagerank}). По заложенным идеям данные работы похожи на уже описанную ранее работу \cite{pagerank}. Однако, в отличие от модели \emph{PageRank}, числовая характеристика важности считается не для вершины графа, а для пары вершин, то есть для ребра. Тем самым, определяется близость между сущностями системы.
    

\hl{Во многих случаях в графы знаний восстановить пропущенные сущности и отношения, отсутствующие в данных.} Например, если предположить, что отношение \emph{БылРожденВСтране} не может быть прописано для каждой сущности, но при этом для всех них известно отношение \emph{БылРожденВГороде}, то отношение \emph{БылРожденВСтране} может быть восстановлено. Для этого также используется отношение \emph{ГородПринадлежитСтране}. 

Хранение информации в виде графов знаний является эффективным способом, поскольку объединяет в себе преимущества тезаурусов, таксономий и онтологий, оставаясь при этом удобным для использования и человеком, и машиной.  Примерами различных графов знаний могут служить DBpedia \cite{dbpedia}, FreeBase \cite{freebase}, YAGO3 \cite{yago3}, Google Knowledge Graph \cite{google_kg}. 

В контексте задач, рассматриваемых в настоящей диссертационной работе, сущностями могут быть, например, ключевые слова, а восстанавливаемым отношениям - отношение синонимии пары слов. Аналогично можно составить графы знаний объектов наукометрической системы с ассоциированными словами, между которыми могут быть отношения \emph{ЯвляетсяУчастником}, \emph{ЯвляютсяСоАвторами}, \emph{ЯвляетсяСотрудником} и т.д..

Далее приводится обзор современных методов восстановления связей в описанных выше графах знаний.

Существует два подхода к решению задачи восстановления отношений в графе знаний, каждое из которых детально описано в \cite{rel_ml}. \hl{Авторы демонстрируют работу программных реализаций различных моделей. Эти модели восстанавливают ребра в графах знаний (например, в Google Knowledge Graph \cite{google_kg})}. Первый из подходов к решению - графовые признаковые модели. Для решения задачи выбирается и подсчитывается некоторая метрика близости между вершинами (такими метриками могут являться локальные меры такие, как число общих соседей, индекс Адамика-Адара и глобальные меры такие, как индекс Катца и другие). Модели, использующий подходы данного семейства, рассматриваются в \cite{Adamic, Barabasi, RePEc, Leicht}. \hl{Второй подход заключается в использовании матричных разложений. Применение таких подходов описывается в работах (\cite{decomp,tens_decomp}).}

Еще одним классом моделей, использующие графовые признаковые представления, являются модели, вычисляющие числовые характеристики по различным путям между парой вершин в графе. Например, в работе \cite{lao} рассматривается множество $\Pi_{L}(i, j, k)$ всевозможных путей длины $L$ между сущностями $e_i, e_j \subset E$ системы. Переменная $k$ означает, что данные сущностями связаны отношением $r_k \in R$. Далее с помощью случайного блуждания появляется возможность оценить вероятность существования каждого из путей между заданными вершинами. Ключевой идеей метода является использование вероятностей различных путей в качестве признакового описания для модели машинного обучения. Далее авторы обучают линейную модель (логистическую регрессию), позволяющую предсказывать вероятности существования отношения между любой парой сущностей. Важное преимущество такой модели заключается в ее интерпретируемости: после того, как программная реализация модели предсказала вероятность существования отношения между сущностями, появляется возможность узнать, на какие пути модель опиралась в большей степени в при подсчете вероятности.

Авторы модели $\emph{PathSim}$ в работах (\cite{pathsim, pathsimext}) используют пути по различным отношениям в графе и на их основе определяет меру близости пары вершин. Эта мера демонстрирует высокий уровень качества на различных исходных данных. Вторая из цитируемых работ дополняет первую. В ней рассматриваются дополнительные внешние источники данных и способы привнести дополнительный сигнал в основную модель, тем самым, улучшив её качество.

Другим расширением идеи, заложенной  в \emph{PathSim}, является модель \emph{W-PathSim}, которая для улучшения качества определения близости между сущностями использует методы тематического моделирования (латентное размещение Дирихле, \cite{lda}).

Второй подход к решению данной задачи заключается в построение латентных векторов для вершин и ребер графа. Как и в задаче определения семантической близости пары слов, в области  восстановления отношений между сущностями в последнее время преобладают методы, строящие векторные представления для сущностей и отношений. 

Примером построения векторного пространства на базе графа знаний может являться модель, построенная в \cite{phd_nickel13}. Автором строится биллинейная форма 

$$ f_{ijk}^{RESCAL} := e_i^TW_ke_j  = \sum_{a=1}^{H_e}\sum_{b=1}^{H_e}w_{abk}e_{ia}e_{jb},$$ 

где $f_{ijk}^{RESCAL}$ - значение близости между $i$-ым и $j$-ым сущностями системы по $k$-ому отношению, $e_i$ - векторное представление размерности $H_e$ для $i$-ой сущности, $W_k$ - матрица параметров для $k$-ого отношения размерности $H_e * H_e$, $w_{abk},e_{ia},e_{ib}$ - компоненты соответсвующих матриц и векторов. При такой постановке видно, что требуется настроить огромное число параметров, даже если размерность векторного представления выбрана относительно небольшого размера.

Некоторые исследователи в своих работах за основу модели берут описанную ранее модель \emph{Word2Vec}. Примерами таких работ могут служить \cite{graph_emb,kg2vec} Графы, используемые в работах, в своих вершинах содержат слова естественного языка. В обоих работах целью является решение некоторой задачи регрессии или классификации на определенных исходных данных, частично или полностью представленных графами.

В первой работе для вершин графа проводится процедура \emph{GraphWalk}. В рамках нее выбирается одна из вершин графа и выполняется переход по случайному его ребру к следующей вершине. Такой процесс проходит некоторое фиксированное число раз, после чего появляется последовательность вершин. Учитывая тот факт, что в вершинах лежит текстовая информация, то фактически получается некоторое предложение естественного языка. Собрав достаточное число предложений, обучаются разные виды модели \emph{Word2Vec}. Далее векторное представление слов  принимается за признаковое описание объектов, после чего появляется возможность обучать известные модели машинного обучения. А именно, в работе были использованы \emph{Support Vector Machines, Linear Regression, Naive Bayes, k-NearestNeighbors}. С помощью этих моделей были решены поставленные задачи регрессии и классификации.

Вторая работа по принятым в ней идеям схожа с первой. Однако здесь отношения между вершинами также задаются словами естественного языка и рассматриваются тройки: исходная вершина, \hl{тип} отношения, конечная вершина. Для этой тройки создается короткое текстовое предложение, после чего происходит аналогичное обучение модели.

В работе \cite{convkg} была построена двухмерная сверточная нейронная сеть с добавлением полносвязных слоев для определения вероятности связи пары сущностей $e_i, e_j$ отношением $r_k$. Другие подходы к построению представлений для \hl{пар сущностей} можно найти в работах \cite{NIPS2013_5028, trouillon16}. 

Авторы популярной работы \cite{transe} предлагают следующую функцию расстояния: $f_r(h, t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||_2^2$, где $h,t$ - субъект и объект, $r$ - отношение, $\mathbf{h},\mathbf{r},\mathbf{t}$ - их векторные представления. Таким образом, авторы пытаются построить такое векторное пространство, в котором под действием отношения субъект будет переходить в точку, максимально близкую к истинному объекту. 

Усовершенствованием предыдущей идее занимались авторы \cite{transr}, которые показали, что пространство отношений не обязано быть таким же, как пространство сущностей и может иметь другую размерность. Для более эффективного вычисления функций близости были введены матрицы проекции, по которым можно было перевести векторное представления из пространства сущностей в пространство отношений. Отмечается, что такой подход усложняет модель \cite{transe}, что требует настройки большего числа параметров и, соответственно, данных для обучения.

В работе \cite{osipov_text_appliance} описывается созданный авторами программно-аппаратный комплекс интеллектуального поиска и анализа больших массивов текстов, именуемый \emph{TextAppliance}. Основной функцией комплекса является семантический поиск текстовых документов в текстовых коллекциях большого объема. Кроме того, реализованы функции реферирования, семантического поиска заимствований, описанного в \cite{osipov_plagiarism}, кросс-языкового поиска, кластеризации документов системы. Перечисленный функции реализован с использованием методов лингвистического анализа, описание которых приводится в \cite{osipov_semantic}. Одной из особенностей \emph{TextAppliance} является способ представления текста в виде неоднородной семантической сети. На этом представлении основываются авторские алгоритмы нечеткого сравнения текстов. Является важным отметить тот факт, что комплекс разработан для обработки больших объемов полнотекстовой информации и многие инженерные решения направлены на эффективную распределенную работу различных модулей сбора, индексации и поиска информации. Следует однако отметить, что в рамках настоящей диссертационной работы не подразумевается наличие полнотекстовой информации, а также интеллектуальных систем большого размера, что делает затруднительным эффективное применение описанных в \cite{osipov_text_appliance,osipov_plagiarism,osipov_semantic} подходов для решения рассматриваемых задач.

\subsection{Графовые методы кластеризации слов естественного языка}

%https://aclanthology.info/pdf/W/W10/W10-2301.pdf

Одной из прикладных задач настоящей диссертации является задача кластеризации ключевых слов больших и сложно организованных аналитических систем. По этой причине были проведены исследования существующих решений в данной области. Обзор основных методов и подходов к решению этой задачи представлен далее.

В работе \cite{word_cluster_web} авторами предлагается графовый подход к кластеризации слов естественного языка. Для построения такого графа используется ресурсы поисковой системы. В процессе работы программной реализации выполняется множество запросов к поисковой системе и фиксируется число найденных документов по каждому запросу. Для пары слов $w_a$, $w_b$ задается 3 запроса: один из них содержит оба этих слова, а второй и третий только слово $w_a$ и только слово $w_b$, соответственно. Имея информацию о количестве документов, релевантных каждому из этих запросов, появляетс возможность подсчета меры близости PMI, которая уже описывалась ранее. Если значение меры выше порогового значения, то в графе слов, который строится авторами, между вершинами $w_a$, $w_b$ будет проходить ребро.

Далее построенный граф кластеризуется алгоритмом, описанным в \cite{clustering_newman}. В ходе работы алгоритм оптимизирует следующий функционал:

$$ Q = \sum_{i} \left(e_{ii} - \left(\sum_{j} e_{ij}\right)^2\right), $$

где суммирование проводится по уже полученным кластерам, $e_{ii}$ обозначает долю ребер в графе, соединяющих вершины $i$-го кластера, а $e_{ij}$ - долю ребер, соединяющих между собой вершины кластеров $i$ и $j$. Данный алгоритм является \emph{агломеративным}. Это означает, что процесс кластеризации начинается с максимального числа кластера (каждая вершина - кластер), после чего кластера объединяются из условия минимизации выписанного выше функционала.

Другой метод, решающий вспомогательную оптимизационную задачу для решения задачи кластеризации, представлен в работе \cite{louvain_modularity}. В этом подходе максимизируется функционал:

$$ Q = \frac{1}{2m}\sum_{i,j}[A_{ij} - \frac{k_i k_j}{2m}]\delta(c_i, c_j), $$

где $\delta$- дельта функция, $A_{ij}$- вес ребра между вершинами $i$ и $j$, $k_i=\sum_j{A_{ij}}, m=\frac{1}{2}\sum_{i,j}A_{ij}$.

Кроме описанных выше агломеративных подходов, существуют \emph{дивизивные} или \emph{дивизионные} методы кластеризации, которые в отличие от агломеративных строят новые кластера путем деления более крупных. Примером дивизивного метода кластеризации является \cite{clustering_mcl}. В ходе работы алгоритма происходит случайное блуждание между вершинами, результатом которого является матрица вероятностей переходов от вершины к вершине. Кроме этого, авторами этой работы были добавлены эвристики, нормирующие нужным образом эту матрицу. По достижении сходимости происходит интерпретация результатов и граф делится на кластера. 

Существует ряд дивизивных алгоритмов кластеризации,  представленных в работе \cite{clustering_mst}. Подходы, используемые в работе основываются на построении \emph{минимального остовного дерева} по графу, а именно дерева, полученного по графу, обладающего свойством минимальной суммы весов ребер.

Следует отметить, что в области графовой кластеризации слов естественного языка важную (а возможно даже решающую) роль играет именно метод построения графа. После этого задача сводится к более общей задаче кластеризации графа. В ходе работы обычно не возникает необходимости в конструировании специфического алгоритма кластеризации. Напротив, гораздо более перспективной выглядит работа в области построения графа, наилучшим образом отражающего семантику стоящих у него в вершинах слов посредством определения ребер между этими вершинами. Когда такой граф построен, естественным выглядит использование одного из существующих и прошедших опробацию алгоритмов кластеризации. Правильный выбор необходимого алгоритма кластеризации также является подзадачей, которую необходимо решить.

\subsubsection{Недостатки существующих решений}
Если рассматривать информационно-аналитическую наукометрическую систему с точки зрения некоторого графа знаний, где сущностями являются, например, научные сотрудники, лаборатории, конференции и между сущностями возникают различные отношения, то существующие и традиционно используемые методы анализа имеют перечисленные далее недостатки.
\begin{itemize}
    \item Такой граф знаний имеет объем, недостаточный для применения существующих латентных и признаковых графовых моделей, описанных в \ref{kg_subsection}. Данные, используемые в этих работах оперируют графами на миллионы и десятки миллионов вершин и ребер.
    \item Для проведения аналитической работы нет необходимости все имеющиеся объекты использовать внутри одного графа. Например, такие сущности, как ключевые слова выглядит разумным вынести в отдельный граф, ребра которого будут иметь только одно отношение <<семантической близости>>. В ином случае может возникнуть ситуация при которой модель хуже обрабатывает более редкие понятия (лаборатории, кафедры, департаменты), потому что большая часть графа <<забита>> ключевыми словами и на них делается основной акцент в обучении.
\end{itemize}

Наличие описанных выше недостатков свидетельствует о необходимости создания методов и подходов, способных определять семантическую связь между объектами по небольшому объему входных данных и различными связями между ними.

\subsection{Выводы из библиографического обзора} \label{related_work_concl}

Подводя итог анализу существующих решений на рассматриваемом направлении, можно выделить тот класс информационно-аналитических систем, для которых такие решения не являются эффективными. Этот класс обладает следующими характеристиками и свойствами.

\begin{itemize}
    \item \textbf{Малые объемы имеющейся информации}. Сложных  модели информационно-аналитических систем обладают большим числом внутренних параметров, которые необходимо оценить по имеющимся данных. Если наблюдается дифицит данных, то эффективно настроить такие модели невозможно. В то же время простые и наивные модели не могут выявить в достаточной мере семантическую информацию из данных.
    \item \textbf{Сущности описываются небольшим объемом текста.} Для лучшего качества желателен именно набор ключевых слов.
    \item \textbf{Системы узкоспециальной направленности.} Для таких систем не подойдут предобученные модели на объемах данных общего назначения.
    \item \textbf{Отсутствие достаточных человекоресурсов для ручного сбора необходимой информации.} Обычно трудозатраты человека для разметки или подготовки данных - очень дорогой ресурс, которым не обладают владельцы небольших информационно-аналитических систем. Более того, зачастую такие люди должны обладать экспертными знаниями. Это факт уменьшает количество такого ресурса и значительно увеличивает его стоимость.
    \item \textbf{Существование дополнительных связей различной природы между сущностями системы.} Кроме того, эти связи могут быть достаточно разреженными.
\end{itemize}

Программный комплекс, разрабатываемый в рамках данной диссертационной работы, призван эффективно решать аналитические задачи в системах данного класса. Для этого к нему предъявляется ряд требований, описанных в приложении \ref{AppendixRequirements}, а также разрабатывается специальная методология исследования, подробно описанная далее.

\section{Методология} \label{methodology}
В данном разделе описывается основные положения методологии построения решений задач, поставленных в настоящей диссертации. Методология строится исходя из недостатков существующих решений подобных задач, а также требований, предъявленных к создаваемому программному комплексу.
Далее по пунктам излагаются важные в выборе общей методологии решения.

\begin{enumerate}
    \item Во многих прикладных областях, связанных с анализом данных, появляется необходимость решения задачи определения семантически близких понятий. При этом объекты, подлежащие сравнению, не хранят в себе информации достаточно для качественного смыслового анализа. Самих объектов, при этом, может быть также не много.

        Для восстановления неявных семантических связей в имеющихся данных естественным выглядит использование \textbf{аппарата теории графов}.
    
        Рассмотрим, например, граф, вершинами которого являются объекты системы, а ребрами - некоторые семантические отношения между объектами. Некоторые из этих ребер могут отсутствовать ввиду недостаточности данных. Восстановить такую связь в некоторых случаях возможно, по связам между другими вершинами. В простейшем случае между парой вершин может существовать путь в графе, что может свидетельствовать о некоторой связи между соответствующими объектами.
    \item Для решения \hl{рассматриваемых в настоящей диссертации} задач используется анализ ключевых слов. Следует отметить, что обычно ключевые слова не имеют никаких ограничений в написании или использовании: по факту, это могут быть произвольные тексты на естественном языке. Пользователи системы могут по-разному вписывать одно и то же слово, употреблять аббревиатуры, переводить термины на другие языки, транслитерировать, допускать опечатки, менять формы слов.

        Использовав аналитические инструменты, можно сделать пространство ключевых слов менее разреженным: \hl{все различные написания одного слова, а также его синонимы связать в общую смысловую единицу и работать сразу с группой слов, а не с каждым словом отдельно.} Кроме того, для анализа необходимо уметь определять, что пара ключевых слов является близкой по смыслу.

        В условиях недостаточности исходных данных, дополнительная семантическая информация о ключевых словах может в значительной степени увеличить точность, а главное полноту моделей определения семантической близости понятий системы. 

        Все перечисленные выше доводы наталкивают на мысль, что перед началом решения основной задачи, \textbf{необходимо решить более низкоуровневую задачу определения семантической близости пары ключевых слов}. Таким образом, решение данной задачи становится \textbf{базовым} шагом для решения основных задач диссертации

    \item Согласно идеям, изложенным в п.1, решение задачи определения семантической близости пары ключевых слов также может быть выполнено с применением \textbf{графовых алгоритмов}.

        Построив граф, в вершинах которого лежат ключевые слова, а ребра задают некоторые отношения, появляется возможность определять семантическую близость не только для тех пар, для которых в графе присутствует ребро, но и для произвольной пары ключевых слов.
        
        Для такой пары анализируются различные характеристики, такие как длина кратчайшего расстояния в графе, поток между парой вершин, меры центральности и другие. Другими словами, в условиях сильной ограниченности в объемах данных появляется возможность восстанавливать семантические связи между двумя вершинами по имеющимся данным о других вершинах. Важной задачей, возникающей при применении таких методов, является задача построения графов, наилучшим образом отображающим семантические отношения между словами.
      
    \item Исходя из постановки основной задачи, и атрибуты системы, и ключевые слова их характеризующие, могут быть связаны дополнительными отношениями. Это обстоятельство предоставляет возможность построить несколько различных графов на одних и тех же наборах вершин. Например, по наукометрической коллекции наборов ключевых слов можно построить графы, в вершинах которых будут сами слова, а ребро будет ставиться в зависимости от выполнения различных условий, к числу которых относятся следующие.
     \begin{itemize}
         \item Пара слов входит в один набор.
         \item Пара слов использовалась одним пользователем во время ввода поискового запроса.
         \item Пара слов, согласно некоторому классификатору, принадлежит общему классу. Классификатор уже разработан и внедрен в рассматриваемую систему. Например, таким классификатором может быть научный рубрикатор.
     \end{itemize}

        Каждая такая идея порождает дополнительный граф, обладающий индивидуальным сигналом, который может помочь в определении семантической близости пары ключевых слов (пары вершин каждого из графов)

        Интеллектуальный анализ набора собранных графов позволяет выделить максимальный объем семантической информации из данных.

    \item Определение методов семантической близости пары слов \textbf{является базовым средством определения близости} более общих понятий - \textbf{наборов ключевых слов}. Здесь принимается естественная гипотеза, что если пара наборов состоит из попарно похожих элементов, то и сами наборы похожи. 
        
            Помимо использования низкоуровневых пословных моделей, имеется возможность построения нового графового представления данных. В этих графах вершинами являются наборы ключевых слов, а ребра указывают на отношения между наборами. Два таких подхода позволяют добиться улучшения качества определения близости наборов ключевых слов.

        \item Как и ранее, за счет \textbf{дополнительных связей} в данных существует возможность создавать \textbf{различные вариации графов}, в вершинах которых уже наборы ключевых слов, а ребра отражают некоторые различные отношения между наборами. Например, пара наборов может иметь \hl{связь} по следующим причинам:
        \begin{itemize}
            \item оба набора содержат одно и то же ключевое слово;
            \item наборы являются наборами ключевых слов к различным публикациям одного автора;
            \item наборы являются наборами ключевых слов двух конференций, на которых выступал один автор.
        \end{itemize}
    \item Поскольку смысловая близость между парой понятий использует множество различных графовых характеристик, которые к тому же подсчитаны по различным графам, то получение искомой формулы близости вручную является очень трудоемкой задачей. Кроме того, необходимость решения такой задачи будет возникать каждый раз при обновлении данных или смене информационной системы.

        В этой связи для преодоления отмеченной трудности задействуется \textbf{аппарат машинного обучения}, который позволяет подбирать необходимую формулу в автоматическом режиме.

    \item Следующим этапом является построение классификаторов и моделей, непосредственно решающие прикладные задачи, \hl{которые лежат в основе данного исследования}. Благодаря разработанным на предыдущих шагах моделях представления информации и методам определения семантической близости, появляется возможность определения близости между объектами информационных систем. Кроме того, различные связи между объектами позволяют определить близость более точно.
\end{enumerate}
%Для решения таких трудностей могут применяться методы, в основе которых лежит теория графов. Процесс решения основан на построении графа, вершинами которого служат слова, а взвешенное ребро определяет некоторое отношение между парой слов. Например, отношением для пары ключевых слов научных публикаций может являться количество публикаций, в которых были указаны одновременно оба рассматриваемых ключевых слова.

Таким образом, для решения поставленных задач выбраны представленные далее положения, определяющие методологию.

\begin{enumerate}
    \item Данные системы представляются в виде множества графов, вершинами которых являются некоторые понятия (ключевые слова/наборы слов/сущности системы), а ребрами - отношения между ними. По построенным графам подсчитываются различные характеристики для пар вершин.
    \item Решается задача определения семантической близости пары ключевых слов. Для этого используются построенные графы, разработанные подходы и технологии машинного обучения. Решению этой задачи посвящена глава \ref{chapt_word_similarity}.
    \item Решается задача определения семантической близости пары наборов ключевых слов. Разработанные модели используют различные графовые представления, подходы и модели, рассмотренные в предыдущих пунктах. Моделям, решающие указанную задачу описаны в главе \ref{chapt_tuple_similarity}.
    \item Используя функцию близости наборов ключевых слов и отношения между сущностями системы, решаются прикладные задачи определения семантической близости пары сущностей. Этой задаче посвящается глава \ref{chapt_applications}.
\end{enumerate}

В дальнейших главах, согласно указанной выше методологии, детально и последовательно описываются разработанные в ходе работы модели, приводятся мотивация выбора того или иного подхода, проводятся тестовые испытания программных реализаций, делаются выводы и рассматриваются планы на будущее по улучшению отдельных компонент.
% графовая близость 

%Важным решения задачи определения семантической близости
\section{Экспертное оценивание качества результатов программных реализаций} \label{experts_introduction}
Для проверки качества полученных результатов в ходе тестовых испытаний в рамках представленного в диссертации исследования зачастую необходима экспертная оценка. Основной причиной такой необходимости является тот факт, что исследования затрагивают сложную и неоднозначную области семантической отношений между объектами. Осложняет процесс оценивания также то обстоятельство, что значительная часть исследования проводилась на данных из определенной специфичной области знания - данных наукометрических систем. Эта область имеет сложную иерархическую структуру, каждое ответвление в которой является узкоспециальным направлением науки. Для таких направлений в открытых источниках не существует достаточного количества знаний об уровне семантической близости от компетентных в данной области людей.

По отмеченным выше причинам в исследованиях, результаты которых описаны в настоящей диссертации, используются специально полученные оценки от группы экспертов, состоящая из кандидатов и докторов физико-математических наук в количестве 8 человек. Экспертам было предложено оценить уровень сементической близости для различных задач, решаемых в рамках работы. Среди задач, качество решения которых оценивалось экспертами, выделяются следующие:
\begin{itemize}
    \item определение уровня семантической близости между парой ключевых слов;
    \item определение уровня семантической близости между парой наборов ключевых слов;
    \item определение уровня семантической близости между парой объктов информационной системы;
    \item определение уровня абстрактности ключевого слова;
    \item релевантность экспертов, определенных поисковым модулем поиска экперта, пользовательскому запросу;
    \item определение кластеров ключевых слов;
    \item определение тематических ключевых слов;
    \item качество построяния тезауруса ключевых слов.
\end{itemize}

Однако, важно отметить то, что для некоторых задач, которые решаются в рамках работы, могут быть частично или полностью протестированы в автоматическом режиме с использованием кажущихся разумными эвристиками, а также открытых наборов данных. Схема проведения таких тестовых испытаний и обоснования их адекватности будут расписана в подразделах <<тестовые испытания>> cоответствующих разделов, посвященных обозначенным выше задачам.

Следующие главы детально описывают разработанную методологию, программную реализацию алгоритмов и соответствующие тестовые испытания. Для проведения значительной части таких испытаний были использованы силы экспертной группы, описанной в данном разделе.

%близость наборов
%Существующие методы имеют ряд недостатков по отношению к решаемой в данной работе задаче. Основная из них - отстутствие наборов данных достаточного объема. Для качественного обучения нейронной сети необходимы миллионы примеров полноценных текстов, в то время как наборы ключевых слов, как правило, состоят лишь из нескольких слов. Доступ к ресурсам поисковых систем является ограниченным и не имея постоянного доступа к ним, сложно получить хорошие результаты. Сложность применения машинного обучения к такого рода задачам - в отсутствии достаточно больших обучающих выборок для тренировки.

%В рамках данной работы представлены методы определения близости по корпусу наборов ключевых слов, также опирающиеся на методы из теории графов. Значительным улучшением является построение второго графа ключевых слов, основанного на контекстной близости пары слов. В следующих далее разделах дано определение контекстной близости для пары ключевых слов, а также представлены методы построения такого графа. После чего показан алгоритмы семантической кластеризации, основанные на введенной мере близости слов. В разделе [?] представлены тестовые данные, результаты экспериментов программных реализаций алгоритмов.

