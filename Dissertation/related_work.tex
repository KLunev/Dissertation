\chapter{Обзор существующих систем анализа ключевых слов} \label{chapt1}

С целью анализа эффективности уже существующих и поиска новых подходов к решению рассматриваемой задачи автором проведены библиографические поисковые исследования, результаты которых представлены в настоящей разделе. 

\section{Методы определения близости между парой слов естественного языка}

% близость слов

Существует большое число общих методов определения похожести пары слов естественного языка. Их можно разделить на методы, не использующие или использующие дополнительные источники информации. К числу первых относятся исторически наиболее ранние и наивные подходы, которые вычисляют близость, не используя никакой дополнительной информации о словах, кроме их непосредственного написания. Одной из основных метрик данного типа является расстояние Левенштейна  (редакторское расстояние, \cite{leven}). Эта метрика подсчитывает количество необходимых операций добавления, удаления или замены одного символа на другой, чтобы из одной строки получить вторую. Традиционно этот алгоритм используется для исправления опечаток: для введенного слова можно найти ближайшие по этой метрике слова из фиксированного словаря. 

Существует ряд более сложных версий алгоритма, в числе которых алгоритм Демерау-Левенштейна \cite{leven_dem}. Усовершенствование этого алгоритма заключается в том, что дополнительно используется четвертая операция транспозиции двух соседних символов.

Несмотря на простоту описанных выше методов, исследования в этом направлении ведутся до сих пор. Данный класс методов может использоваться в более сложных и совершенных моделях в качестве дополнительных источников для определения близости. Примером более сложной модели в данном направлении является модель редакторского расстояния с настроенными стоимостями для операций вставки, удаления и замены символов.
Авторы \cite{learn_leven} с помощью разработанных алгоритмов и обучающей выборки определяют стоимость замены одного символа на другой (а также добавления и удаления каждого символа). Авторы высказывают гипотезу о том, что различные замены символов не должны иметь один и тот же вес: если человек опечатался, то весьма вероятно, что он ввел символ, который находится близко к правильному символу на клавиатуре. Такая замена не должна сильно влиять на общее расстояние метрике. Другим примером важности индивидуального подбора весов замен под каждый символ могут являться безударные гласные: люди чаще путают при написании пару букв <<а>> и <<о>>, чем, например, пару букв <<а>> и <<е>>. Важным достоинством такого алгоритма является то, что слово и его транслитерированная версия (например, <<компьютер>>-<<computer>>) становятся близки по данному расстоянию. В то же время, недостатком является необходимость обучающей коллекцию различных написаний одного слова.

Следующий важный этап развития идеи редакторского расстояния заключается в использовании контекста. В работе \cite{context_leven} авторы настраивают стоимости переходов между символами с учетом контекста. Выдвигается гипотеза о том, что стоимость замены одного символа на другого может сильно зависеть от символов, которые стоят рядом с заменяемым символов и символом-заменителем. Например, удаление символа <<ь>> более обоснованно в конце глаголов, так как ошибки <<ться>>/<<тся>> частотны и вероятнее всего подразумевал одно из слов, написав другое. Как и в предыдущей работе, данный алгоритм требует обучающую выборку, но в данном случае ее размер должен быть значительно больше, поскольку число параметров растет экспоненциально с увеличением размера рассматриваемого контекста.

Еще одной разновидностью метрик на строках является расстояние Джаро — Винклера (\cite{jaro1,jaro2}). Эта метрика подсчитывает минимальное число односимвольных преобразований, которое необходимо для того, чтобы изменить одно слово в другое и использовалась для сравнения написаний имен в Бюро переписи населения США.

Ряд авторов рассматривают слово как множество символов (или как множество символьных n-грамм - последовательностей из n подряд идущих символов) и далее определяют близость между словами, как близость между соответствующими множествами, либо как близость между соответсвующими векторами, на $i$-ой позиции в которой стоит единица, если данная n-грамма присутствует в слове и ноль - в противном случае. Примером метрики на векторах может являться алгоритм q-grams (\cite{qgrams}), в ходе работы которого подсчитывается число совпавших n-грамм. Другими метриками могут служить,  расстояние Жаккара или косинусное расстоения, приведенное, например, в \cite{sim_metr}.

%Другая идея в области использования мер близости на строках для определения смысловой близости -- использование 
Другим направлением исследований в области определения смысловой близости пары слов является использование фонетической информации рассматриваемых слов. Примерами таких работ могут служить \cite{soundex,phone_sim}. Работы опираются на гипотезу о том, что похожие слова могут звучать одинаково. Авторы первой работы по слову строят его короткий код таким образом, чтобы различные слова с одним кодом звучали похоже. Авторы второй работы предлагают различные алгоритмы определения фонетической близости, в том числе они используют расстояние Левенштейна на фонемах для рассматриваемых слов.
%, авторы которых при использовании мер близости на строках для определения смысловой близости между понятиями

Таким образом, данное направление позволяет строить метрики близости, основанные исключительно на написании конкретных слов. Данный класс методов позволяет достаточно эффективно решать задачу исправления опечаток, но имеет множество недостатков. Основной из них заключается в том, что при его использовании не учитывается семантика слов. Это обстоятельство существенно сужает круг прикладных задач, для решения которых эти методы можно было бы применить. 

Существенного улучшения качества определения близости между парой ключевых слов можно добиться, используя дополнительные знания о словах. Это могут быть тексты, в которых слова употреблены, коллекции ключевых слов, информация об объектах, к которым эти слова приписаны, вручную составленные тезаурусы и словари. Каждое из этих направлений имеет свои преимущества и недостатки, анализ которых приведен далее.

Одно из направлений определения близости пар ключевых слов с использованием вспомогательных данных - изучение частот использования рассматриваемых слов в различных коллекциях текстовых документов. Базовым методом определения близости пары ключевых слов в рамках таких исследований является сбор информации о совместной встречаемости слов внутри одного набора. Факт появления пары слов в одном предложении или тексте может быть важным сигналом для определения смысловой близости. Методы, основанные на этой идее, разбираются в \cite{freq_1,freq_2,pmi}. Более совершенные на этом направлении алгоритмы основаны на вычислении взаимной информации (Pointwise mutual information, PMI), введенной авторами [7]. Использование таких алгоритмов позволяет получить решение об уровне близости пар слов  не только по их совместной встречаемости, но и путем учета частоты встречаемости каждого из слов в коллекции. Согласно этой метрике, высокое значение семантической близости имеют пары слов, которые часто встречаются вместе и редко поодиночке. 

Недостатком описанных выше статистических методов является необходимость сбора коллекции данных большого размера, поскольку значения PMI и подобных ему статистических метрик сильно неустойчивы.  Зачастую эта особенность приводит к тому, что наиболее близкими парами в смысле этой меры близости являются те, которые встретились единственный раз в одном общем документе. Для того, чтобы уменьшить негативный эффект на практике, принято исключать пары слов, которые встретились в корпусе меньше некоторого порогового значения. Другой способ обойти сложившуюся трудность в ином определении вероятностей появления каждого из слов, а также вероятности их совместного появления. Для этого служат методики сглаживания вероятности, принятые в области построения языковых моделей. Основные способы сглаживания представлены в работе \cite{lm}. Также существуют усовершенствования PMI меры различными эвристическими предположениями: усредненная и средневзвешенная взаимная информация (average and weighted average mutual information), рассмотренные, соответственно, в \cite{avg_pmi} и \cite{w_avg_pmi}; контекстная усредненная взаимная информация (contextual average mutual information), введенная в \cite{context_pmi}; нормированная взаимная информация (normalized mutual information), введенная в \cite{npmi}, квадратичная и кубическая взаимная информация (PMI2 и PMI3), рассмотренные в \cite{pmi23}.  В этих публикациях отмечается, что сам контекст, в котором употребляются слова, используется в самом примитивном виде, а именно, в данном контексте проверяется факт наличия обоих рассматриваемых слов. Остальные слова контекста никак не учитываются, что является существенным недостатком описанных выше подходов.

В другой работе \cite{search_eng} вопрос вычисления семантической близости решается с помощью поисковых систем. Программа запрашивает пару сравниваемых слов через открытый API и получает совместную и индивидуальные частоты встречаемостей слов в интернете. На основе этой информации подсчитывается уровень похожести слов друг на друга. Очевидным недостатком такого подхода является ограниченная поисковой системой пропускная способность (количество запросов в единицу времени) и общее количество запрос.

Различные вариации PMI-метрик являются, по сути, вероятностными методами, поскольку подразумевают вычисления оценки вероятности встретить каждое из понятий, а также эту пару понятий совместно внутри одного текста. Существуют и другие вероятностные методы сравнения пары слов естественного языка. Например, может быть использован $\chi^2$ критерий и тест отношения правдоподобия. Способ применения данных методов описан в \cite{freq_est_overview}

Важной особенностью в применении PMI-подобных метрик к набору текстов является то, что они не показывают смысловую близость между понятиями в явном виде, а скорее определяют коллокации: <<Российская Федерация>>, <<крейсер Аврора>>, <<завод имени Кирова>>, <<средний класс>>, <<пластическая операция>> и другие. Тем не менее, знание того, что совместная встречаемость пары понятий внутри одного множества слов (предложение, документ, набор ключевых слов, короткое описание объекта и т.д.) статистически значимо превосходит случаи их отдельных появлений, является важным фактором для определения в том числе и семантической близости между этими понятиями.

Многие подходы к решению задачи определения близости пары слов используют понятие n-граммы.
Cимвольной/пословной n-граммой называют последовательность фиксированной длины из определенного числа подряд идущих символов/слов. Символьные и пословные n-граммы широко используются в различных задачах из области обработки естественного языка таких как построение языковых моделей \cite{ngrams_1,ngrams_2,ngrams_3,ngrams_4} и моделей машинного перевода \cite{ngrams_mt_1,ngrams_mt_2,ngrams_mt_3}. Недостатком n-граммных моделей является так называемое проклятие размерности, которое в данном случае говорит о том, что при увеличении длины n-граммы катастрофически быстро растет число возможных n-грамм данного размера, а также параметров системы, что делает затруднительным их применение во многих случаях. Также при работе с n-граммами необходимы объемы текстов огромных размеров. Если предметная область, в которой решается задача, является узкоспециальной, то получение данных достаточного объема зачастую является невозможным.

В работе \cite{ngrams_sim} авторами предложены различные метрики близости на основе n-граммного представления слов.

В работе \cite{Albatineh2011} авторы используют  меру Жаккара для определение близости пары ключевых слов: каждому ключевому слову ставится в соответствии множество понятий и для определения близости вычисляются размеры объединения и пересечения этих множеств. Отмечается, что как только словам в однозначное соответствие поставлены некоторые множества, сразу становится возможным вычисление близости на основании различных мер близости пары множеств. Помимо меры Жаккара, существует чуть менее популярная мера Серенсена (\cite{dice_1}). Эти и другие меры близости на множествах подробно описаны в \cite{dist_between_sets}.

Авторы \cite{Shirude} для определения близости используют комбинацию из трех моделей определения близости: n-граммную модель, модель близости жаккара, а также модель векторного пространства. Последняя подразумевает представление слов в виде вектора определенной длины. Близость в свою очередь сводится к величине скалярного произведения векторов для двух слов. Эта модель детально описывается в \cite{vector_space}. Имея три функции близости, авторы вычисляют среднее по их значениям. Это приводит к тому, что такая композиция уменьшает влияние слабых сторон каждого отдельного алгоритма. Тем не менее такой наивный метод комбинирования может даже ухудшать результат, если входящие в нее модели демонстрируют низкое качество.

Важной особенностью алгоритмов, основанных на вычислении символьной n-граммной близости, расстояния левенштейна и наибольшей общей подпоследовательности, является то, что такие методы не дают представления о смысловой близости между словами и способны определять близкие слова только по похожести написания. Это является серьезным недостатком для задач, в которых важна смысловая близость между понятиями. Примером такой задачи может быть разработка классической поисковой системы, где удачное добавление синонимов для слов запроса, сформулированного пользователем, может вылиться в более релевантную выдачу для этого запроса.  Несмотря на этот недостаток, данные методы могут быть применены внутри более сложных алгоритмов для повышения их качества.

Существует класс методов, решающих задачу семантической близости пар слов с помощью готовых тезаурусов, словарей или других семантических сетей. Важным источником знаний об отношениях между словами английского языка является семантическая сеть WordNet (\cite{wordnet}). Слова в данной сети могут быть связаны одним из нескольких отношений: гипероним, гипоним, <<имеет участника>> (факультет-профессор), <<является участником>> (пилот-экипаж), мероним, антоним. Также имеются лексические, антонимические, контекстные связи между словами. Для русского языка существует несколько аналогов: RussNet (\cite{russnet}), YARN (\cite{yarn, yarn_2}), RuThes (\cite{ruthes}), Russian WordNet (\cite{russian_wordnet}).  Чтобы посчитать близость по таким тезаурусам, строится дерево, в вершинах которого стоят слова (вершина в \textsc{WordNet} является синсетом - множеством слов, не отличимых по смыслу), а ребра указывают на отношение гиперонимии между парой вершин. Таким образом, в листьях дерева лежат узкоспециальные понятия, которые обобщаются их предками в дереве, в корне же лежит слово наиболее общего значения. Имея такое дерево, появляется возможность вычислять смысловую близость понятий по их взаимному расположению внутри этого дерева. Так авторы \cite{wordnet_sim_0} в своей формуле близости используют глубину наиболее конкретного по значению предка, а авторы  \cite{wordnet_sim_1} в дополнении к этому считают расстояние между вершинами. Чем больше расстояние между словами и чем глубже находится общий предок, тем меньше уровень смысловой похожести. В работах \cite{wordnet_hybrid_1,wordnet_hybrid_2} используются гибридные методы определения близости: расстояния и глубина в дереве, вероятности встречаемостей в корпусах, признаки, основанные на свойствах слов в рассматриваемых тезаурусах. Недостаток тезаурусных подходов в их неполноте, а также в том, что некоторые из них не являются публично доступными. Кроме того, тезаурусы обычно охватывают общий домен и существует мало словарей для специфических областей.

Еще одним открытым источником отношений между понятиями является интернет-энциклопедия wikipedia (www.wikipedia.org). Данная база позволяет эффективно использовать категории, ссылки, полные тексты и мета-данные статей для извлечения семантической информации о словах. Авторы \cite{wiki} строят различные меры близости, опираясь на тексты статей и представляя сравниваемые понятия в виде векторов определенной длины. В \cite{wiki_2} автор использует данные википедии (в частности используются информация о ссылках между статьями) и разработанные им метрики близости слов для решения задачи снятия лексической неоднозначности. 

С развитием вычислительной техники большой популярностью начинают пользоваться методы, основанные на обучении нейронный сетей. Одними из самых известных методов определения семантической близости слов является модели \textsc{word2vec} (\cite{word2vec}) и \textsc{GloVe} (\cite{glove}). Модель \textsc{word2vec} представляет собой нейронную сеть, на вход которой подаются огромные корпуса текстовых данных. Задачей обучения является построение такого векторного представления для текущего слова (\textsc{word embeddings}), которое максимально точно способно предсказать рядом стоящие в тексте слова. Обученная модель строит векторное пространство, обладающее рядом полезных свойств, которые в наше время широко используются для решения многих задач естественного языка, связанных с семантической информацией. Одним из таких свойств - семантическая близость понятий, векторные представления которых похожи. Таким образом любую пару слов из словаря можно сравнить, использую, например, косинусное расстояние между векторами.

Мощной моделью построения векторных представлений для слов является модель GloVe, которая строит матрицу частотностей встречаемостей слов во всех возможных контекстах. Далее используются методы уменьшения размерности пространства, которые оставляют только наиболее значимые компоненты в разложении. В то время, как \textsc{Word2Vec} является предиктивной моделью, \textsc{GloVe} представляет собой модель на основе подсчета статистики.

Данные методы являются очень эффективными, но требуют огромные наборы данных для обучения моделей. Это обстоятельство делает их неприменимыми к задачам, в которых полные тексты документов недоступны. К их числу которых принадлежит задача определения семантической близости пары ключевых слов научных публикаций. Эти методы зачастую определяют также контекстную близость, по определению которой два слова близки, если они встречаются в похожих контекстах. Во многих практических задачах подобного эффекта использования метрики близости хочется избежать, поскольку, например, слова “Математика” и “Физика” могут встречаться в одних и тех же контекстах, но как пара ключевых слова для научных публикаций эти слова явно не являются  семантически близкими.

Различные методы векторного представления описаны и протестированы на открытых источниках в работе \cite{embed_1}. Несмотря на высокое качество определения семантической близости моделей, использование полнотекстовой информации существенно ограничивает область применения данных методов, посколько для многих прикладных задач не имеется достаточного количества текстовой информации. Возникает трудность при работе в узкоспециальных областях: модели, обученные на корпусах общего назначения, не могут улавливать особенности таких областей. Использование текстовых данных из рассматриваемой области для обучения ведет к неправильной настройке параметров модели и недообучению, по причине недостатка этих самых данных. Это выливается в низкий уровень качества моделей. 

При исследовании области семантической близости пары ключевых слов возникает дополнительная информация о наборах, в которые входят рассматриваемые слова. Помимо этого зачастую для набора известен также объект, к которому этот набор приписан. Авторы \cite{folk} вводят понятие фолксономии. Фолксономией называется кортеж $(U, T, R, Y)$, где $U,T$ и $R$ - конечные множества, элементами которых служат, соответственно, пользователи, ключевые слова и ресурсы. $Y$ - тернарное отношение между ними, т.е. $Y  \subseteq U \times T \times R$. Постом называется тройка $(u, T_{ur}, r)$, где $u \in U, r \in R$, $T_{ur}$ - непустое множество ключевых слов такое, что $T_ur  \coloneqq {t \in T | (u, t, r) \in Y}$. Авторы \cite{folk_2} считают близость между ключевыми словами несколькими способами. Первый способ заключается в построении меры близости по статистике совместной встречаемости пары ключевых слов. Второй способ предполагает построение векторного пространства для каждого слова. На $i-$ой позиции стоит количество документов, в которые одновременно входит рассматриваемое ключевое слово и $i-$ое. Далее мера близости вводится как косинусное расстояние между векторами в этом пространстве. Последний способ, который описывается в \cite{folk} подсчитывает меру, подобную мере \textsc{PageRank} (\cite{pagerank}) для документов в сети Веб.

Многие прикладные области определения семантической похожести между словами сталкиваются с проблемой недостатка данных для качественного определения уровня близости. Такими данными обычно служат полнотекстовые документы, которые определяют контекст для слов, подлежащих сравнению. Для решения таких задач могут применяться методы, в основе которых лежит теория графов. Процесс решения основан на построении графа, вершинами которого служат слова, а взвешенное ребро определяет некоторое отношение между парой слов. Например, отношением для пары ключевых слов научных публикаций может являться количество публикаций, в которых были указаны одновременно оба рассматриваемых ключевых слова. Построив граф ключевых слов по заданному отношению, появляется возможность определять семантическую близость не только для тех пар, для которых в графе присутствует ребро, но и для произвольной пары ключевых слов. Для такой пары анализируются различные характеристики, такие как длина кратчайшего расстояния в графе, поток между парой вершин и другие. Другими словами в условиях сильной ограниченности в объемах данных появляется возможность восстанавливать семантические связи между двумя вершинами по имеющимся данным о других вершинах. Важной задачей, возникающей при применении таких методов, является задача построения графа, наилучшим образом отображающим семантические отношения между словами.

   %В [PAPER] авторы проводят вычисление близости, основываясь на теоретико-графовых алгоритмах. Рассматривается граф, вершинами которого являются ключевые слова, а ребра показывают факт принадлежности пары слов одному набору. Далее по построенному графу для пары вершин  вычисляются различные характеристики: расстояние, количество кратчайших путей, различные графовые меры близости. Недостатком описанной в статье модели является тот факт, что смысловая близость между парой слов стремительно падает при увеличении расстояния в графе. Это происходит по той причине, что набор ключевых слов далеко не всегда состоит из близких по смыслу слов. Поэтому при переходе от одной вершины к другой, вероятность исходной вершины быть близкой по смыслу к другой стремительно уменьшается. В работе [word2vec] для определения контекстной близости между словами естественного языка авторы обучают по корпусу текстов нейронную сеть, представляющую каждое слово в виде вектора не очень большой длины. Имея такое представление, уровень близости пары слов может быть вычислен, как мера близости между векторами, например, с помощью косинусной меры. В работах [] представлены подходы решения задачи с помощью классических методов машинного обучения с учителем. По паре текстов вычисляются вручную разработанные факторы, которые, по мнению авторов, сильнее всего влияют на уровень близости. После чего на этих факторах и обучающем множестве пар документов тренируется модель машинного обучения. В более старых работах близость [] вычисляется при помощи статистической меры TFIDF. Каждому слову документа в соответствии ставится число, которое тем больше, чем чаще это слово встречается в данном документе и реже в других документах. 

% графовая близость 

%Важным решения задачи определения семантической близости



%близость наборов
%Существующие методы имеют ряд недостатков по отношению к решаемой в данной работе задаче. Основная из них - отстутствие наборов данных достаточного объема. Для качественного обучения нейронной сети необходимы миллионы примеров полноценных текстов, в то время как наборы ключевых слов, как правило, состоят лишь из нескольких слов. Доступ к ресурсам поисковых систем является ограниченным и не имея постоянного доступа к ним, сложно получить хорошие результаты. Сложность применения машинного обучения к такого рода задачам - в отсутствии достаточно больших обучающих выборок для тренировки.

%В рамках данной работы представлены методы определения близости по корпусу наборов ключевых слов, также опирающиеся на методы из теории графов. Значительным улучшением является построение второго графа ключевых слов, основанного на контекстной близости пары слов. В следующих далее разделах дано определение контекстной близости для пары ключевых слов, а также представлены методы построения такого графа. После чего показан алгоритмы семантической кластеризации, основанные на введенной мере близости слов. В разделе [?] представлены тестовые данные, результаты экспериментов программных реализаций алгоритмов.

