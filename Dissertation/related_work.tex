\chapter{\hl{Методы и средства анализа информации с использованием ключевых слов}} \label{chapt_related_work}
\hl{В данной главе рассматриваются известные подходы к решению задач в направлении выявления семантической информации, проводится их анализ, выявляются недостатки. После чего автором описывается использованная в настоящей диссертации методология. Эта методология призвана решить недостатки в существующих моделях определения смысловой близости объектов информационных систем по их ключевым словам.}

\hl{В первом разделе проводится библиографический обзор существующих методов, выделяются сильные и слабые стороны моделей. Анализу подвергаются работы, в которых исследуются семантическая близость для пар слов и пар коротких предложений естественного языка. В частности, обозреваются работы, связанные с анализом ключевых слов.}

\hl{Второй раздел содержит в себе выводы из обзора и указывает на недостатки рассмотренных методов и на причины неприменимости этих методов к решаемым в данной работе задачам.}

\hl{В заключающем разделе главы вводится методология, используемая автором в последующих главах для разработки моделей семантической близости. Показывается целесообразность и необходимость рассмотренной методологии, ее преимущества в сравнении с уже существующими. Стоит также отметить, что отдельные пункты методологии представляют собой самостоятельный интерес.}

\section{\hl{Библиографический обзор}}

\hl{С целью анализа эффективности уже существующих и поиска новых подходов к решению рассматриваемой задачи автором проведены библиографические поисковые исследования, результаты которых представлены в настоящей разделе.}

\subsection{\hl{Методы определения близости между парой слов естественного языка}}

% близость слов

Существует большое число общих методов определения похожести пары слов естественного языка. Их можно разделить на методы, не использующие или использующие дополнительные источники информации. К числу первых относятся исторически наиболее ранние и наивные подходы, которые вычисляют близость, не используя никакой дополнительной информации о словах, кроме их непосредственного написания. Одной из основных метрик данного типа является расстояние Левенштейна  (редакторское расстояние, \cite{leven}). Эта метрика подсчитывает количество необходимых операций добавления, удаления или замены одного символа на другой, чтобы из одной строки получить вторую. Традиционно этот алгоритм используется для исправления опечаток: для введенного слова можно найти ближайшие по этой метрике слова из фиксированного словаря. 

Существует ряд более сложных версий алгоритма, в числе которых алгоритм Демерау-Левенштейна \cite{leven_dem}. Усовершенствование этого алгоритма заключается в том, что дополнительно используется четвертая операция транспозиции двух соседних символов.

Несмотря на простоту описанных выше методов, исследования в этом направлении ведутся до сих пор. Данный класс методов может использоваться в более сложных и совершенных моделях в качестве дополнительных источников для определения близости. Примером более сложной модели в данном направлении является модель редакторского расстояния с настроенными стоимостями для операций вставки, удаления и замены символов.
Авторы \cite{learn_leven} с помощью разработанных алгоритмов и обучающей выборки определяют стоимость замены одного символа на другой (а также добавления и удаления каждого символа). Авторы высказывают гипотезу о том, что различные замены символов не должны иметь один и тот же вес: если человек опечатался, то весьма вероятно, что он ввел символ, который находится близко к правильному символу на клавиатуре. Такая замена не должна сильно влиять на общее расстояние метрике. Другим примером важности индивидуального подбора весов замен под каждый символ могут являться безударные гласные: люди чаще путают при написании пару букв <<а>> и <<о>>, чем, например, пару букв <<а>> и <<е>>. Важным достоинством такого алгоритма является то, что слово и его транслитерированная версия (например, <<компьютер>>-<<computer>>) становятся близки по данному расстоянию. В то же время, недостатком является необходимость обучающей коллекцию различных написаний одного слова.

Следующий важный этап развития идеи редакторского расстояния заключается в использовании контекста. В работе \cite{context_leven} авторы настраивают стоимости переходов между символами с учетом контекста. Выдвигается гипотеза о том, что стоимость замены одного символа на другого может сильно зависеть от символов, которые стоят рядом с заменяемым символов и символом-заменителем. Например, удаление символа <<ь>> более обоснованно в конце глаголов, так как ошибки <<ться>>/<<тся>> частотны и вероятнее всего подразумевал одно из слов, написав другое. Как и в предыдущей работе, данный алгоритм требует обучающую выборку, но в данном случае ее размер должен быть значительно больше, поскольку число параметров растет экспоненциально с увеличением размера рассматриваемого контекста.

Еще одной разновидностью метрик на строках является расстояние Джаро — Винклера (\cite{jaro1,jaro2}). Эта метрика подсчитывает минимальное число односимвольных преобразований, которое необходимо для того, чтобы изменить одно слово в другое и использовалась для сравнения написаний имен в Бюро переписи населения США.

Ряд авторов рассматривают слово как множество символов (или как множество символьных n-грамм - последовательностей из n подряд идущих символов) и далее определяют близость между словами, как близость между соответствующими множествами, либо как близость между соответсвующими векторами, на $i$-ой позиции в которой стоит единица, если данная n-грамма присутствует в слове и ноль - в противном случае. Примером метрики на векторах может являться алгоритм q-grams (\cite{qgrams}), в ходе работы которого подсчитывается число совпавших n-грамм. Другими метриками могут служить,  расстояние Жаккара или косинусное расстоения, приведенное, например, в \cite{sim_metr}.

%Другая идея в области использования мер близости на строках для определения смысловой близости -- использование 
Другим направлением исследований в области определения смысловой близости пары слов является использование фонетической информации рассматриваемых слов. Примерами таких работ могут служить \cite{soundex,phone_sim}. Работы опираются на гипотезу о том, что похожие слова могут звучать одинаково. Авторы первой работы по слову строят его короткий код таким образом, чтобы различные слова с одним кодом звучали похоже. Авторы второй работы предлагают различные алгоритмы определения фонетической близости, в том числе они используют расстояние Левенштейна на фонемах для рассматриваемых слов.
%, авторы которых при использовании мер близости на строках для определения смысловой близости между понятиями

Таким образом, данное направление позволяет строить метрики близости, основанные исключительно на написании конкретных слов. Данный класс методов позволяет достаточно эффективно решать задачу исправления опечаток, но имеет множество недостатков. Основной из них заключается в том, что при его использовании не учитывается семантика слов. Это обстоятельство существенно сужает круг прикладных задач, для решения которых эти методы можно было бы применить. 

Существенного улучшения качества определения близости между парой ключевых слов можно добиться, используя дополнительные знания о словах. Это могут быть тексты, в которых слова употреблены, коллекции ключевых слов, информация об объектах, к которым эти слова приписаны, вручную составленные тезаурусы и словари. Каждое из этих направлений имеет свои преимущества и недостатки, анализ которых приведен далее.

Одно из направлений определения близости пар ключевых слов с использованием вспомогательных данных - изучение частот использования рассматриваемых слов в различных коллекциях текстовых документов. Базовым методом определения близости пары ключевых слов в рамках таких исследований является сбор информации о совместной встречаемости слов внутри одного набора. Факт появления пары слов в одном предложении или тексте может быть важным сигналом для определения смысловой близости. Методы, основанные на этой идее, разбираются в \cite{freq_1,freq_2,pmi}. Более совершенные на этом направлении алгоритмы основаны на вычислении взаимной информации (Pointwise mutual information, PMI), введенной авторами [7]. Использование таких алгоритмов позволяет получить решение об уровне близости пар слов  не только по их совместной встречаемости, но и путем учета частоты встречаемости каждого из слов в коллекции. Согласно этой метрике, высокое значение семантической близости имеют пары слов, которые часто встречаются вместе и редко поодиночке. 

Недостатком описанных выше статистических методов является необходимость сбора коллекции данных большого размера, поскольку значения PMI и подобных ему статистических метрик сильно неустойчивы.  Зачастую эта особенность приводит к тому, что наиболее близкими парами в смысле этой меры близости являются те, которые встретились единственный раз в одном общем документе. Для того, чтобы уменьшить негативный эффект на практике, принято исключать пары слов, которые встретились в корпусе меньше некоторого порогового значения. Другой способ обойти сложившуюся трудность в ином определении вероятностей появления каждого из слов, а также вероятности их совместного появления. Для этого служат методики сглаживания вероятности, принятые в области построения языковых моделей. Основные способы сглаживания представлены в работе \cite{lm}. Также существуют усовершенствования PMI меры различными эвристическими предположениями: усредненная и средневзвешенная взаимная информация (average and weighted average mutual information), рассмотренные, соответственно, в \cite{avg_pmi} и \cite{w_avg_pmi}; контекстная усредненная взаимная информация (contextual average mutual information), введенная в \cite{context_pmi}; нормированная взаимная информация (normalized mutual information), введенная в \cite{npmi}, квадратичная и кубическая взаимная информация (PMI2 и PMI3), рассмотренные в \cite{pmi23}.  В этих публикациях отмечается, что сам контекст, в котором употребляются слова, используется в самом примитивном виде, а именно, в данном контексте проверяется факт наличия обоих рассматриваемых слов. Остальные слова контекста никак не учитываются, что является существенным недостатком описанных выше подходов.

В другой работе \cite{search_eng} вопрос вычисления семантической близости решается с помощью поисковых систем. Программа запрашивает пару сравниваемых слов через открытый API и получает совместную и индивидуальные частоты встречаемостей слов в интернете. На основе этой информации подсчитывается уровень похожести слов друг на друга. Очевидным недостатком такого подхода является ограниченная поисковой системой пропускная способность (количество запросов в единицу времени) и общее количество запрос.

Различные вариации PMI-метрик являются, по сути, вероятностными методами, поскольку подразумевают вычисления оценки вероятности встретить каждое из понятий, а также эту пару понятий совместно внутри одного текста. Существуют и другие вероятностные методы сравнения пары слов естественного языка. Например, может быть использован $\chi^2$ критерий и тест отношения правдоподобия. Способ применения данных методов описан в \cite{freq_est_overview}

Важной особенностью в применении PMI-подобных метрик к набору текстов является то, что они не показывают смысловую близость между понятиями в явном виде, а скорее определяют коллокации: <<Российская Федерация>>, <<крейсер Аврора>>, <<завод имени Кирова>>, <<средний класс>>, <<пластическая операция>> и другие. Тем не менее, знание того, что совместная встречаемость пары понятий внутри одного множества слов (предложение, документ, набор ключевых слов, короткое описание объекта и т.д.) статистически значимо превосходит случаи их отдельных появлений, является важным фактором для определения в том числе и семантической близости между этими понятиями.

Многие подходы к решению задачи определения близости пары слов используют понятие n-граммы.
Cимвольной/пословной n-граммой называют последовательность фиксированной длины из определенного числа подряд идущих символов/слов. Символьные и пословные n-граммы широко используются в различных задачах из области обработки естественного языка таких как построение языковых моделей \cite{ngrams_1,ngrams_2,ngrams_3,ngrams_4} и моделей машинного перевода \cite{ngrams_mt_1,ngrams_mt_2,ngrams_mt_3}. Недостатком n-граммных моделей является так называемое проклятие размерности, которое в данном случае говорит о том, что при увеличении длины n-граммы катастрофически быстро растет число возможных n-грамм данного размера, а также параметров системы, что делает затруднительным их применение во многих случаях. Также при работе с n-граммами необходимы объемы текстов огромных размеров. Если предметная область, в которой решается задача, является узкоспециальной, то получение данных достаточного объема зачастую является невозможным.

В работе \cite{ngrams_sim} авторами предложены различные метрики близости на основе n-граммного представления слов.

В работе \cite{Albatineh2011} авторы используют  меру Жаккара для определение близости пары ключевых слов: каждому ключевому слову ставится в соответствии множество понятий и для определения близости вычисляются размеры объединения и пересечения этих множеств. Отмечается, что как только словам в однозначное соответствие поставлены некоторые множества, сразу становится возможным вычисление близости на основании различных мер близости пары множеств. Помимо меры Жаккара, существует чуть менее популярная мера Серенсена (\cite{dice_1}). Эти и другие меры близости на множествах подробно описаны в \cite{dist_between_sets}.

Авторы \cite{Shirude} для определения близости используют комбинацию из трех моделей определения близости: n-граммную модель, модель близости жаккара, а также модель векторного пространства. Последняя подразумевает представление слов в виде вектора определенной длины. Близость в свою очередь сводится к величине скалярного произведения векторов для двух слов. Эта модель детально описывается в \cite{vector_space}. Имея три функции близости, авторы вычисляют среднее по их значениям. Это приводит к тому, что такая композиция уменьшает влияние слабых сторон каждого отдельного алгоритма. Тем не менее такой наивный метод комбинирования может даже ухудшать результат, если входящие в нее модели демонстрируют низкое качество.

Важной особенностью алгоритмов, основанных на вычислении символьной n-граммной близости, расстояния левенштейна и наибольшей общей подпоследовательности, является то, что такие методы не дают представления о смысловой близости между словами и способны определять близкие слова только по похожести написания. Это является серьезным недостатком для задач, в которых важна смысловая близость между понятиями. Примером такой задачи может быть разработка классической поисковой системы, где удачное добавление синонимов для слов запроса, сформулированного пользователем, может вылиться в более релевантную выдачу для этого запроса.  Несмотря на этот недостаток, данные методы могут быть применены внутри более сложных алгоритмов для повышения их качества.

Существует класс методов, решающих задачу семантической близости пар слов с помощью готовых тезаурусов, словарей или других семантических сетей. Важным источником знаний об отношениях между словами английского языка является семантическая сеть WordNet (\cite{wordnet}). Слова в данной сети могут быть связаны одним из нескольких отношений: гипероним, гипоним, <<имеет участника>> (факультет-профессор), <<является участником>> (пилот-экипаж), мероним, антоним. Также имеются лексические, антонимические, контекстные связи между словами. Для русского языка существует несколько аналогов: RussNet (\cite{russnet}), YARN (\cite{yarn, yarn_2}), RuThes (\cite{ruthes}), Russian WordNet (\cite{russian_wordnet}).  Чтобы посчитать близость по таким тезаурусам, строится дерево, в вершинах которого стоят слова (вершина в \textsc{WordNet} является синсетом - множеством слов, не отличимых по смыслу), а ребра указывают на отношение гиперонимии между парой вершин. Таким образом, в листьях дерева лежат узкоспециальные понятия, которые обобщаются их предками в дереве, в корне же лежит слово наиболее общего значения. Имея такое дерево, появляется возможность вычислять смысловую близость понятий по их взаимному расположению внутри этого дерева. Так авторы \cite{wordnet_sim_0} в своей формуле близости используют глубину наиболее конкретного по значению предка, а авторы  \cite{wordnet_sim_1} в дополнении к этому считают расстояние между вершинами. Чем больше расстояние между словами и чем глубже находится общий предок, тем меньше уровень смысловой похожести. В работах \cite{wordnet_hybrid_1,wordnet_hybrid_2} используются гибридные методы определения близости: расстояния и глубина в дереве, вероятности встречаемостей в корпусах, признаки, основанные на свойствах слов в рассматриваемых тезаурусах. Недостаток тезаурусных подходов в их неполноте, а также в том, что некоторые из них не являются публично доступными. Кроме того, тезаурусы обычно охватывают общий домен и существует мало словарей для специфических областей.

Еще одним открытым источником отношений между понятиями является интернет-энциклопедия wikipedia (www.wikipedia.org). Данная база позволяет эффективно использовать категории, ссылки, полные тексты и мета-данные статей для извлечения семантической информации о словах. Авторы \cite{wiki} строят различные меры близости, опираясь на тексты статей и представляя сравниваемые понятия в виде векторов определенной длины. В \cite{wiki_2} автор использует данные википедии (в частности используются информация о ссылках между статьями) и разработанные им метрики близости слов для решения задачи снятия лексической неоднозначности. 

С развитием вычислительной техники большой популярностью начинают пользоваться методы, основанные на обучении нейронный сетей. Одними из самых известных методов определения семантической близости слов является модели \textsc{word2vec} (\cite{word2vec}) и \textsc{GloVe} (\cite{glove}). Модель \textsc{word2vec} представляет собой нейронную сеть, на вход которой подаются огромные корпуса текстовых данных. Задачей обучения является построение такого векторного представления для текущего слова (\textsc{word embeddings}), которое максимально точно способно предсказать рядом стоящие в тексте слова. Обученная модель строит векторное пространство, обладающее рядом полезных свойств, которые в наше время широко используются для решения многих задач естественного языка, связанных с семантической информацией. Одним из таких свойств - семантическая близость понятий, векторные представления которых похожи. Таким образом любую пару слов из словаря можно сравнить, использую, например, косинусное расстояние между векторами.

Мощной моделью построения векторных представлений для слов является модель GloVe, которая строит матрицу частотностей встречаемостей слов во всех возможных контекстах. Далее используются методы уменьшения размерности пространства, которые оставляют только наиболее значимые компоненты в разложении. В то время, как \textsc{Word2Vec} является предиктивной моделью, \textsc{GloVe} представляет собой модель на основе подсчета статистики.

Данные методы являются очень эффективными, но требуют огромные наборы данных для обучения моделей. Это обстоятельство делает их неприменимыми к задачам, в которых полные тексты документов недоступны. К их числу которых принадлежит задача определения семантической близости пары ключевых слов научных публикаций. Эти методы зачастую определяют также контекстную близость, по определению которой два слова близки, если они встречаются в похожих контекстах. Во многих практических задачах подобного эффекта использования метрики близости хочется избежать, поскольку, например, слова “Математика” и “Физика” могут встречаться в одних и тех же контекстах, но как пара ключевых слова для научных публикаций эти слова явно не являются  семантически близкими.

Различные методы векторного представления описаны и протестированы на открытых источниках в работе \cite{embed_1}. Несмотря на высокое качество определения семантической близости моделей, использование полнотекстовой информации существенно ограничивает область применения данных методов, посколько для многих прикладных задач не имеется достаточного количества текстовой информации. Возникает трудность при работе в узкоспециальных областях: модели, обученные на корпусах общего назначения, не могут улавливать особенности таких областей. Использование текстовых данных из рассматриваемой области для обучения ведет к неправильной настройке параметров модели и недообучению, по причине недостатка этих самых данных. Это выливается в низкий уровень качества моделей. 

При исследовании области семантической близости пары ключевых слов возникает дополнительная информация о наборах, в которые входят рассматриваемые слова. Помимо этого зачастую для набора известен также объект, к которому этот набор приписан. Авторы \cite{folk} вводят понятие фолксономии. Фолксономией называется кортеж $(U, T, R, Y)$, где $U,T$ и $R$ - конечные множества, элементами которых служат, соответственно, пользователи, ключевые слова и ресурсы. $Y$ - тернарное отношение между ними, т.е. $Y  \subseteq U \times T \times R$. Постом называется тройка $(u, T_{ur}, r)$, где $u \in U, r \in R$, $T_{ur}$ - непустое множество ключевых слов такое, что $T_ur  \coloneqq {t \in T | (u, t, r) \in Y}$. Авторы \cite{folk_2} считают близость между ключевыми словами несколькими способами. Первый способ заключается в построении меры близости по статистике совместной встречаемости пары ключевых слов. Второй способ предполагает построение векторного пространства для каждого слова. На $i-$ой позиции стоит количество документов, в которые одновременно входит рассматриваемое ключевое слово и $i-$ое. Далее мера близости вводится как косинусное расстояние между векторами в этом пространстве. Последний способ, который описывается в \cite{folk} подсчитывает меру, подобную мере \textsc{PageRank} (\cite{pagerank}) для документов в сети Веб.

\subsubsection{\hl{Недостатки существующих решений}}
\hl{Основные недостатки лучших из существующих подходов для определения семантической близости пары слов естественного языка}:
\begin{itemize}
    \item \hl{сложные модели определения близости требуют больших объемов полнотекстовой информации. Информационно-аналитические системы зачастую имеют на порядки меньшие объемы данных, что делает невозможным обучение таких моделей;}
    \item \hl{использование готовых моделей не позволяет использовать семантическую специфику конкретной информационной системы. Другими словами, пара семантически близких понятий одной системы может не являться таковой во второй. Например, пара слов <<вычислительная математика>> и <<теория чисел>> могут считаться похожими в системах общего назначения, как два направления в математике. В то же время, для более узкоспециальной наукометрической системы эти понятия не должны быть слишком близки семантически;}
    \item \hl{простые модели не способны восстанавливать сложные семантические связи в данных;}
    \item \hl{существующие модели не используют в достаточной мере дополнительную информацию об отношениях между сущностями системы;}
\end{itemize}

\subsection{\hl{Методы определения близости между парой наборов ключевых слов}}
\subsection{\hl{Методы определения близости между объектами в графах знаний}} \label{kg_subsection}
\hl{Переходя на более высокий и целостностный уровень, можно отметить, что некоторые исследования в области семантической близости были проделаны и на этапе определения близости объектов информационной системы. Более того, исследователями зачастую решалась общая задача восстановления отношений между объектами, что не всегда равносильно наличию смысловой близости между ними. }

\hl{Возможны различные способы хранения данных информационной системы, одним из самых современных на момент написания является граф знаний. Графы знаний - направленные графы с подписанными ребрами (отношениями) между вершинами (сущностями). При этом количество возможных типов отношений может быть огромным и исчисляться тысячами.}

\hl{Во многих случаях в таких графах возникают естественные излишки между имеющимися отношениями. Это нередко позволяет восстановить пропущенные сущности и отношения в таких графах. Например, если предположить, что отношение \emph{БылРожденВСтране} не может быть прописано для каждой сущности, но при этом для всех них известно отношение \emph{БылРожденВГороде}, то отношение \emph{БылРожденВСтране} может быть восстановлено. Для этого также используется отношение \emph{ГородПринадлежитСтране}. }

\hl{Хранение информации в виде графов знаний является удобным, объединяет в себе преимущества тезаурусов, таксономий и онтологий, оставаясь удобным для использования и человеком, и машиной.}

\hl{Примерами различных графов знаний могут служить DBpedia \cite{dbpedia}, FreeBase \cite{freebase}, YAGO3 \cite{yago3}, Google Knowledge Graph \cite{google_kg}. }

\hl{В контексте задач, рассматриваемых в настоящей диссертационной работе, сущностями могут быть, например, ключевые слова, а восстанавливаемым отношениям - отношение синонимии пары слов. Аналогично можно составить графы знаний объектов наукометрической системы с ассоциированными словами, между которыми могут быть отношения \emph{ЯвляетсяУчастником}, \emph{ЯвляютсяСоАвторами}, \emph{ЯвляетсяСотрудником} и т.д..}

\hl{Далее приводится обзор современных методов восстановления связей в описанных выше графах знаний.}

\hl{Существует два подхода к решению задачи восстановления отношений в графе знаний, каждое из них детально описано в \cite{rel_ml}. Первый из подходов - графовые признаковые модели. Для решения задачи выбирается и подсчитывается некоторая метрика близости между вершинами (такими метриками могут являться локальные меры такие, как число общих соседей, индекс Адамика-Адара и глобальные меры такие, как индекс Катца и другие). Модели, использующий подходы данного семейства, рассматриваются в \cite{Adamic, Barabasi, RePEc, Leicht}. }

\hl{Второй подход к решению данной задачи заключается в построение латентных векторов для вершин и ребер графа. Как и в задаче определения семантической близости пары слов, в области  восстановления отношений между сущностями в последнее время преобладают методы, строящие векторные представления для сущностей и отношений. }

\hl{Примером построения векторного пространства на базе графа знаний может являться модель, построенная в \cite{phd_nickel13}. Автором строится биллинейная форма }

$$ f_{ijk}^{RESCAL} := e_i^TW_ke_j  = \sum_{a=1}^{H_e}\sum_{b=1}^{H_e}w_{abk}e_{ia}e_{jb},$$ 

\hl{где $f_{ijk}^{RESCAL}$ - значение близости между $i$-ым и $j$-ым сущностями системы по $k$-ому отношению, $e_i$ - векторное представление размерности $H_e$ для $i$-ой сущности, $W_k$ - матрица параметров для $k$-ого отношения размерности $H_e * H_e$, $w_{abk},e_{ia},e_{ib}$ - компоненты соответсвующих матриц и векторов. При такой постановке видно, что требуется настроить огромное число параметров, даже если размерность векторного представления выбрана относительно небольшого размера.}

\hl{Некоторые исследователи в своих работах за основу модели берут описанную ранее модель \emph{Word2Vec}. Примерами таких работ могут служить \cite{graph_emb,kg2vec} Графы, используемые в работах, в своих вершинах содержат слова естественного языка. В обоих работах целью является решение некоторой задачи регрессии или классификации на определенных исходных данных, частично или полностью представленных графами.}

\hl{В первой работе для вершин графа проводится процедура \emph{GraphWalk}. В рамках нее выбирается одна из вершин графа и выполняется переход по случайному его ребру к следующей вершине. Такой процесс проходит некоторое фиксированное число раз, после чего появляется последовательность вершин. Учитывая тот факт, что в вершинах лежит текстовая информация, то по факту, получается некоторое предложение естественного языка. Собрав достаточное число предложений, обучаются разные виды модели \emph{Word2Vec}. Далее векторное представление слов  принимается за признаковое описание объектов, после чего появляется возможность обучать известные модели машинного обучения. А именно, в работе были использованы \emph{Support Vector Machines, Linear Regression, Naive Bayes, k-NearestNeighbors}. С помощью этих моделей были решены поставленные задачи регрессии и классификации.}

\hl{Вторая работа по заложенным в нее идеям схожа с первой, но здесь отношения между вершинами также задаются словами естественного языка и рассматриваются тройки: исходная вершина, отношение, конечная вершина. Для этой тройки создается короткое текстовое предложение, после чего происходит аналогичное обучение модели }

\hl{В работе \cite{convkg} была построена двухмерная сверточная сеть с добавлением полносвязных слоев для определения вероятности связи пары сущностей $e_i, e_j$ отношением $r_k$. Другие подходы к построению представлений для сущностей и отношений можно найти в работах \cite{NIPS2013_5028, trouillon16}. }

\hl{Авторы популярной работы \cite{transe} предлагают следующую функцию расстояния: $f_r(h, t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||_2^2$, где $h,t$ - субъект и объект, $r$ - отношение, $\mathbf{h},\mathbf{r},\mathbf{t}$ - их векторные представления. Таким образом, авторы пытаются построить такое векторное пространство, в котором под действием отношения субъект будет переходить в точку, максимально близкую к истинному объекту. }

\hl{Усовершенствованием предыдущей идее занимались авторы \cite{transr}, которые показали, что пространство отношений не обязано быть таким же, как пространство сущностей и может иметь другую размерность. Для возможности вычисления функций близости были введены матрицы проекции, по которым можно было перевести векторное представления из пространства сущностей в пространство отношений. Отмечается, что такой подход усложняет модель \cite{transe}, что требует настройки большего числа параметров и, соответственно, данных для обучения.}

\subsubsection{\hl{Недостатки существующих решений}}
\hl{Если рассматривать информационную наукометрическую систему с точки зрения некоторого графа знаний, где сущностями являются, например, научные сотрудники, лаборатории, конференции и между сущностями возникают различные отношения, то существующие методы анализа имеют следующий ряд недостатков:}
\begin{itemize}
    \item \hl{такой граф знаний имеет объем, совершенно недостаточный для применения существующих латентных и признаковых графовых моделей, описанных в \ref{kg_subsection}. Данные, используемые в этих работах оперируют графами на миллионы и десятки миллионов вершин и ребер;}
    \item \hl{дла проведения аналитической работы нет необходимости все имеющиеся объекты использовать внутри одного графа. Например, такие сущности, как ключевые слова выглядит разумным вынести в отдельный граф, ребра которого будут иметь только одно отношение <<семантической близости>>. В ином случае может возникнуть ситуация при которой модель хуже обрабатывает более редкие понятия (лаборатории, кафедры, департаменты), потому что большая часть графа <<забита>> ключевыми словами и на них делается основной акцент в обучении.}
\end{itemize}

\hl{Наличие описанных выше недостатков говорит о необходимости создания методов и подходов, способных определять семантическую связи между объектами по небольшому объему входных данных и различными связями между ними.}

\subsection{\hl{Выводы из библиографического обзора}} \label{related_work_concl}

\hl{Подводя итог анализу существующих решений, можно выделить тот класс систем, для которых известные методы не являются эффективными. Этот класс обладает следующими характеристиками и свойствами:}

\begin{itemize}
    \item \hl{\textbf{малые объемы имеющийся информации}. Сложных  модели обладают большим числом внутренних параметров, которые необходимо оценить по имеющимся данных. Если наблюдается дифицит данных, то эффективно настроить такие модели невозможно. В то же время простые и наивные модели не могут выявить в достаточной мере семантическую информацию из данных;}
    \item \hl{\textbf{сущности описываются небольшим объемом текста.} Для лучшего качества желателен именно набор ключевых слов;}
    \item \hl{\textbf{системы узкоспециальной направленности.} Для таких систем не подойдут предобученные модели на объемах данных общего назначения;}
    \item \hl{\textbf{отсутствие достаточных человекоресурсов для ручного сбора необходимой информации.} Обычно трудозатраты человека для разметки или подготовки данных - очень дорогой ресурс, которым не обладают владельцы небольших информационно-аналитических систем. Более того, зачастую такие люди должны обладать экспертными знаниями. Это факт уменьшает количество такого ресурса и значительно увеличивает его стоимость;}
    \item \hl{\textbf{существование дополнительных связей различной природы между сущностями системы.} Кроме того, эти связи могут быть достаточно разреженными.}
\end{itemize}

\hl{Программный комплекс, разрабатываемый в рамках данной диссертационной работы, призван эффективно решать аналитические задачи в системах данного класса. Для этого к нему предъявляется ряд требований, описанных в приложении \ref{AppendixRequirements}, а также разрабатывается специальная методология исследования, подробно описанная далее.}

\section{Методология} \label{methodology}
\hl{В данном разделе описывается принцип построения решения задач, решаемых в рамках настоящей диссертации. Методология строится исходя из недостатков существующих решений подобных задач, а также требований, предъявленных к создаваемому программному комплексу.}
\hl{Далее по пунктам излагаются важные в выборе общей методологии решения.}

\begin{enumerate}
    \item \hl{Во многих прикладных областях, связанных с анализом данных, встает задача определения семантически близких понятий. При этом объекты, подлежащие сравнению, не хранят в себе информации достаточно для качественного смыслового анализа. Самих объектов, при этом, может быть также не много.}

        \hl{Для восстановления неявных семантических связей в имеющихся данных выглядит естественным использование \textbf{аппарата теории графов}.}
    
        \hl{Рассмотрим, например, граф, вершинами которого являются объекты системы, а ребрами - некоторые семантические отношения между объектами. Некоторые из этих ребер могут отсутствовать ввиду недостаточности данных. Восстановить такую связь в некоторых случаях возможно, по связам между другими вершинами. В простейшем случае между парой вершин может существовать путь в графе, что может свидетельствовать о некоторой связи между соответсвующими объектами.}
    \item \hl{Для решения поставленных задач используется анализ ключевых слов. Стоит отметить, что обычно ключевые слова не имеют никаких ограничений в написании или использовании: по факту, это могут быть произвольные тексты на естественном языке. Пользователи системы могут по-разному вписывать одно и то же слово, употреблять аббревиатуры, переводить термины на другие языки, транслитерировать, допускать опечатки, менять формы слов.}

        \hl{Использовав аналитические инструменты, можно сделать пространство ключевых слов менее разреженным: для каждого слова прикрепить различные варианты его написания, а также расположить поблизости семантически похожие на него слова. Кроме того, для анализа необходимо уметь определять, что пара ключевых слов является близкой по смыслу.}

        \hl{В условиях недостаточности исходных данных, дополнительная семантическая информация о ключевых словах может в значительной степени увеличить точность, а главное полноту моделей определения семантической близости понятий системы. }

        \hl{Все это наталкивает на мысль, что перед началом решения основной задачи, \textbf{необходимо решить более низкоуровневую задачу определения семантической близости пары ключевых слов}. Таким образом, решение данной задачи становится \textbf{базовым} шагом для решения основных задач диссертации}

    \item \hl{Согласно идеям, изложенным в п.1, решение задачи определения семантической близости пары ключевых слов также может быть выполнено с применением \textbf{графовых алгоритмов}.}

        \hl{Построив граф, в вершинах которого лежат ключевые слова, а ребра задают некоторые отношения, появляется возможность определять семантическую близость не только для тех пар, для которых в графе присутствует ребро, но и для произвольной пары ключевых слов.}
        
        \hl{Для такой пары анализируются различные характеристики, такие как длина кратчайшего расстояния в графе, поток между парой вершин, меры центральности и другие. Другими словами, в условиях сильной ограниченности в объемах данных появляется возможность восстанавливать семантические связи между двумя вершинами по имеющимся данным о других вершинах. Важной задачей, возникающей при применении таких методов, является задача построения графов, наилучшим образом отображающим семантические отношения между словами.}
      
   \item \hl{Исходя из постановки задачи, и сущности системы, и ключевые слова их характеризующие, могут быть связаны дополнительными отношениями. Это дает возможность построить несколько различных графов на одних и тех же наборах вершин. Например, по наукометрической коллекции наборов ключевых слов можно построить графы, в вершинах которых будут сами слова, а ребро будет ставиться в зависимости от выполнения различных условий:}
     \begin{itemize}
         \item \hl{пара слов входит в один набор;}
         \item \hl{пара слов использовалась одним пользователем во время ввода поискового запроса;}
         \item \hl{пара слов, согласно некоторому классификатору, принадлежит общему классу. Классификатор уже разработан и внедрен в рассматриваемую систему. Например, таким классификатором может быть научный рубрикатор.}
     \end{itemize}

        \hl{Каждая такая идея порождает дополнительный граф, обладающий индивидуальным сигналом, который может помочь в определении семантической близости пары ключевых слов (пары вершин каждого из графов)}

        \hl{Интеллектуальный анализ множества собранных графов позволяют выделить максимальное семантической информации из данных.}

    \item \hl{Определение методов семантической близости пары слов \textbf{является базовым средством определения близости} более общих понятий: \textbf{наборов ключевых слов}. Здесь принимается естественная гипотеза, что если пара наборов состоит из попарно похожих элементов, то и сами наборы похожи. }
        
            \hl{Помимо использования низкоуровневых пословных моделей, имеется возможность построения нового графового представления данных. В этих графах вершинами являются наборы ключевых слов, а ребра указывают на отношения между наборами. Два таких подхода позволяют добиться улучшения качества определения близости наборов ключевых слов.}

        \item \hl{Как и ранее, засчет \textbf{дополнительных связей} в данных существует возможность создавать \textbf{различные вариации графов}, в вершинах которых уже наборы ключевых слов, а ребра отражают некоторые различные отношения между наборами. Например, пара наборов может иметь в разных графах по следующим причинам:}
        \begin{itemize}
            \item \hl{оба набора содержат одно и то же ключевое слово;}
            \item \hl{наборы являются наборами ключевых слов к различным публикациям одного автора;}
            \item \hl{наборы являются наборами ключевых слов двух конференций, на которых выступал один автор.}
        \end{itemize}
    \item \hl{Поскольку смысловая близость между парой понятий использует множество различных графовых характеристик, которые к тому же подсчитаны по различным графам, то получение искомой формулы близости вручную является очень трудоемкой задачей. Кроме того, такая задача будет вставать каждый раз при обновлении данных или смене информационной системы.}

           \hl В этой связи для решения данной трудности задействуется \textbf{аппарат машинного обучения}, который позволяет подбирать необходимую формулу в автоматическом режиме.

    \item \hl{Следующим этапом является построение классификаторов и моделей, непосредственно решающие поставленные в диссертации задачи. Благодаря разработанным на предыдущих шагах моделях представления информации и методам определения семантической близости, появляется возможность определения близости между объектами информационных систем. Кроме того, различные связи между объектами позволяют определить близость более точно.}
\end{enumerate}
%\hl{Для решения таких трудностей могут применяться методы, в основе которых лежит теория графов. Процесс решения основан на построении графа, вершинами которого служат слова, а взвешенное ребро определяет некоторое отношение между парой слов. Например, отношением для пары ключевых слов научных публикаций может являться количество публикаций, в которых были указаны одновременно оба рассматриваемых ключевых слова.}

\hl{Таким образом, для решения поставленных задач представляется следующая методология:}

\begin{enumerate}
    \item \hl{данные системы представляются в виде множества графов, вершинами которых являются некоторые понятия (ключевые слова/наборы слов/сущности системы), а ребрами - отношения между ними. По построенным графам подсчитываются различные характеристики для пар вершин;}
    \item \hl{решается задача определения семантической близости пары ключевых слов. Для этого используются построенные графы, разработанные подходы и технологии машинного обучения. Решению этой задачи посвящена глава \ref{chapt_word_similarity};}
    \item \hl{решается задача определения семантической близости пары наборов ключевых слов. Разработанные модели используют различные графовые представления, подходы и модели, рассмотренные в предыдущих пунктах. Моделям, решающие указанную задачу описаны в главе \ref{chapt_tuple_similarity}.}
    \item \hl{используя функцию близости наборов ключевых слов и отношения между сущностями системы, решаются прикладные задачи определения семантической близости пары сущностей. Этой задаче посвящается глава \ref{chapt_applications}}
\end{enumerate}

\hl{В дальнейших главах, согласно указанной выше методологии, детально и последовательно описываются разработанные в ходе работы модели, приводятся мотивация выбора того или иного подхода, проводятся тестовые испытания программных реализаций, делаются выводы и рассматриваются планы на будущее по улучшению отдельных компонент.}
% графовая близость 

%Важным решения задачи определения семантической близости



%близость наборов
%Существующие методы имеют ряд недостатков по отношению к решаемой в данной работе задаче. Основная из них - отстутствие наборов данных достаточного объема. Для качественного обучения нейронной сети необходимы миллионы примеров полноценных текстов, в то время как наборы ключевых слов, как правило, состоят лишь из нескольких слов. Доступ к ресурсам поисковых систем является ограниченным и не имея постоянного доступа к ним, сложно получить хорошие результаты. Сложность применения машинного обучения к такого рода задачам - в отсутствии достаточно больших обучающих выборок для тренировки.

%В рамках данной работы представлены методы определения близости по корпусу наборов ключевых слов, также опирающиеся на методы из теории графов. Значительным улучшением является построение второго графа ключевых слов, основанного на контекстной близости пары слов. В следующих далее разделах дано определение контекстной близости для пары ключевых слов, а также представлены методы построения такого графа. После чего показан алгоритмы семантической кластеризации, основанные на введенной мере близости слов. В разделе [?] представлены тестовые данные, результаты экспериментов программных реализаций алгоритмов.

